import tempfile
import numpy as np
import streamlit as st
from models import load_file_and_split, create_vector_store, rag_chain
from langchain.chat_models import ChatOpenAI
from sklearn.metrics.pairwise import cosine_similarity
from langchain.embeddings import OpenAIEmbeddings

import os
os.environ['OPENAI_API_KEY']='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'


def extract_question_intent_and_keywords(user_input):
    import re
    keywords = re.findall(r'\w+', user_input)
    intent = "ì •ë³´ ìš”ì²­" if "ì•Œë ¤ì¤˜" in user_input or "ì„¤ëª…" in user_input else "ê¸°íƒ€"
    return {"keywords": keywords, "intent": intent}


def filter_retrieved_documents_with_intent(user_query, retrieved_texts):
    analysis = extract_question_intent_and_keywords(user_query)
    keywords = analysis["keywords"]
    filtered_texts = [
        text for text in retrieved_texts if any(keyword.lower() in text.lower() for keyword in keywords)
    ]
    return filtered_texts if filtered_texts else retrieved_texts


def evaluate_retrieval_consistency(retrieved_texts, model_response, user_query):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ìƒì„±ëœ ì‘ë‹µ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    embedding_model = OpenAIEmbeddings()
    
    
    if not retrieved_texts:
        return None

    
    retrieved_embeddings = embedding_model.embed_documents(retrieved_texts)
    response_embedding = embedding_model.embed_query(model_response)
    query_embedding = embedding_model.embed_query(user_query)

    
    query_similarities = cosine_similarity([query_embedding], retrieved_embeddings).flatten()
    response_similarities = cosine_similarity([response_embedding], retrieved_embeddings).flatten()

    avg_query_similarity = np.mean(query_similarities) if query_similarities.size > 0 else 0
    max_response_similarity = max(response_similarities) if response_similarities.size > 0 else 0

    return (avg_query_similarity + max_response_similarity) / 2


def generate_intelligent_prompt(user_input, retrieved_texts, mode="basic"):
    """
    ë¬¸ì„œ ë‚´ìš©ê³¼ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if mode == "basic":
        return f"ì§ˆë¬¸: {user_input}\nìš´ì „ì ë³´í—˜ ê´€ë ¨ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."

    elif mode == "engineered":
        if not retrieved_texts:
            return (
                f"ë¬¸ì„œì— ê´€ë ¨ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ "
                f"'ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤."
            )
        context = "\n".join(retrieved_texts[:3])  # ìƒìœ„ 3ê°œì˜ ë¬¸ì„œ í…ìŠ¤íŠ¸
        return (
            f"ì•„ë˜ëŠ” ìš´ì „ì ë³´í—˜ê³¼ ê´€ë ¨ëœ ë¬¸ì„œì…ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì„ ë°˜ë“œì‹œ ê¸°ë°˜ìœ¼ë¡œ "
            f"ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì— ëŒ€í•´ ìœ ì¶”í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
            f"ë¬¸ì„œ ë‚´ìš©:\n{context}\n\n"
            f"ì§ˆë¬¸: {user_input}\n"
            f"ë‹µë³€:"
        )
    else:
        raise ValueError("Invalid mode. Choose 'basic' or 'engineered'.")


st.title("ğŸ—¨ï¸ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")
st.sidebar.title("ì„¤ì •")


if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None
if "history" not in st.session_state:
    st.session_state.history = []
if "default_model" not in st.session_state:
    st.session_state.default_model = ChatOpenAI(model_name="gpt-4")
if "processed_file" not in st.session_state:
    st.session_state.processed_file = ""
if "response_mode" not in st.session_state:
    st.session_state.response_mode = "basic"  
if "similarity_scores" not in st.session_state:
    st.session_state.similarity_scores = {"basic": None, "engineered": None}


uploaded_file = st.sidebar.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

if uploaded_file:
    if st.session_state.processed_file != uploaded_file.name:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
            temp_file.write(uploaded_file.read())
            temp_file_path = temp_file.name

        with st.spinner("íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
            chunks = load_file_and_split(temp_file_path)
            vector_store = create_vector_store(uploaded_file.name, chunks)
            st.session_state.qa_chain = rag_chain(vector_store)
            st.session_state.processed_file = uploaded_file.name
        st.sidebar.success(f"íŒŒì¼ `{uploaded_file.name}` ì²˜ë¦¬ ì™„ë£Œ!")
    else:
        st.sidebar.info(f"íŒŒì¼ `{uploaded_file.name}`ì€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")


st.session_state.response_mode = st.sidebar.radio(
    "ì‘ë‹µ ëª¨ë“œ ì„ íƒ:",
    options=["basic", "engineered"],
    format_func=lambda x: "ê¸°ë³¸ í”„ë¡¬í”„íŠ¸" if x == "basic" else "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§",
    horizontal=True
)


if st.session_state.history:
    for chat in st.session_state.history:
        with st.chat_message("user"):
            st.markdown(chat["user"])
        with st.chat_message("assistant"):
            if st.session_state.response_mode == "basic":
                st.markdown(chat["basic_answer"])
            else:
                st.markdown(chat["engineered_answer"])


def handle_user_input():
    user_input = st.session_state.get("dynamic_user_input", "").strip()
    if user_input:
        if st.session_state.qa_chain:
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                # ê²€ìƒ‰ ë° í•„í„°ë§
                intent_analysis = extract_question_intent_and_keywords(user_input)
                preprocessed_query = " ".join(intent_analysis["keywords"])
                retrieved_docs = st.session_state.qa_chain.retriever.get_relevant_documents(preprocessed_query)
                retrieved_texts = [doc.page_content for doc in retrieved_docs]
                filtered_texts = filter_retrieved_documents_with_intent(user_input, retrieved_texts)


                basic_prompt = generate_intelligent_prompt(user_input, filtered_texts, mode="basic")
                basic_answer = st.session_state.qa_chain.run(basic_prompt)

                engineered_prompt = generate_intelligent_prompt(user_input, filtered_texts, mode="engineered")
                engineered_answer = st.session_state.qa_chain.run(engineered_prompt)

                
                basic_score = evaluate_retrieval_consistency(filtered_texts, basic_answer, user_input)
                engineered_score = evaluate_retrieval_consistency(filtered_texts, engineered_answer, user_input)

        
                st.session_state.similarity_scores = {
                    "basic": basic_score if basic_score is not None else 0.0,
                    "engineered": engineered_score if engineered_score is not None else 0.0,
                }

        else:
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                basic_answer = st.session_state.default_model.predict(f"ì§ˆë¬¸: {user_input}\nìš´ì „ì ë³´í—˜ ê´€ë ¨ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.")
                engineered_answer = "ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
                filtered_texts = []

            
                st.session_state.similarity_scores = {"basic": 0.0, "engineered": 0.0}

        st.session_state.history.append(
            {
                "user": user_input,
                "basic_answer": basic_answer,
                "engineered_answer": engineered_answer,
            }
        )
        st.session_state["dynamic_user_input"] = ""

st.text_input(
    "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
    value="",
    key="dynamic_user_input",
    placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  Enter í‚¤ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.",
    on_change=handle_user_input,
    label_visibility="collapsed",
)


if st.session_state.similarity_scores["basic"] is not None:
    st.sidebar.write(f"ğŸ” Basic Prompt Score: {st.session_state.similarity_scores['basic']:.4f}")
if st.session_state.similarity_scores["engineered"] is not None:
    st.sidebar.write(f"ğŸ” Engineered Prompt Score: {st.session_state.similarity_scores['engineered']:.4f}")


if st.sidebar.button("ëŒ€í™” ì´ˆê¸°í™”"):
    st.session_state.history = []
    st.session_state.response_mode = "basic"
    st.rerun()
