{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhV7way62D4q"
   },
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B34G2Q7b7978",
    "outputId": "5819b3ea-58a7-4b9d-a6b9-c076c4045b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "KhFaDH2_2Hq4",
    "outputId": "382c3175-6dbe-419d-fc9c-3f8cb8ac934f"
   },
   "outputs": [],
   "source": [
    "# !pip install pypdf\n",
    "# !pip install openai\n",
    "# !pip install pandas\n",
    "# !pip install requests\n",
    "# !pip install chromadb\n",
    "# !pip install langchain\n",
    "# !pip install pdfplumber\n",
    "# !pip install transformers\n",
    "# !pip install python-dotenv\n",
    "# !pip install langchain-community\n",
    "# !pip install sentence-transformers\n",
    "# !pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pgVK_YsG8oRL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PDFPlumberLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irPvGaL-1gN_"
   },
   "source": [
    "# 데이터 로드 및 전처리(PDF 처리 및 벡터 스토어 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "U5nMFfGG1X9M"
   },
   "outputs": [],
   "source": [
    "# 데이터 로드, PDF 처리 및 텍스트 청크 생성\n",
    "def load_and_split_pdf(path_ins, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    PDF 파일을 로드하고 텍스트를 청크로 나눔\n",
    "    \"\"\"\n",
    "    # PDF 로더 초기화\n",
    "    loader = PDFPlumberLoader(path_ins)\n",
    "    pages = loader.load()\n",
    "\n",
    "    # 데이터 전처리\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators = ['\\n\\n', '\\n', ' ', ''],   # \\n\\n: 문단, \\n: 새로운 줄을 구분, ' ': 공백, '' (빈 문자열): 공백\n",
    "        chunk_size = chunk_size,                # 최대 1000자의 텍스트를 포함\n",
    "        chunk_overlap = chunk_overlap           # 각 청크는 마지막 200자의 텍스트가 겹친다. 이전 데이터를 확인해 문맥을 매끄럽게 하기 위해서\n",
    "        )\n",
    "\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# PDF 데이터를 벡터 스토어로 저장\n",
    "def process_pdf_to_vectorstore(vectorstore_name, chunks):\n",
    "    \"\"\"\n",
    "    PDF 데이터를 청크로 나누어 벡터 스토어에 저장\n",
    "    \"\"\"\n",
    "    # 청크 내용 추출\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "    # 메타데이터 및 ID 생성\n",
    "    metadatas = [{'page': chunk.metadata.get('page', 'unknown')} for chunk in chunks]\n",
    "    ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "    # 허깅페이스 임베딩 모델 로드\n",
    "    model_name = 'jhgan/ko-sroberta-multitask'\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    # 벡터 스토어 초기화 및 데이터 추가\n",
    "    vector_store = Chroma.from_texts(\n",
    "        texts=texts,\n",
    "        embedding=embedding_model,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "    # 벡터 스토어 저장\n",
    "    vector_store.persist()\n",
    "    print(f\"PDF 데이터를 벡터 스토어 '{vectorstore_name}'에 저장 완료!\")\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bRoSOWQhw-d",
    "outputId": "ff9fee59-0e69-4f19-eff2-8147df290ccf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74069/2951013322.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
      "/home/sangho/miniconda3/envs/pro3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 데이터를 벡터 스토어 'Korean_PDF_HuggingFace_Embeddings'에 저장 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74069/2951013322.py:47: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_store.persist()\n"
     ]
    }
   ],
   "source": [
    "# PDF 파일 경로 및 벡터 스토어 이름\n",
    "path_ins = \"/home/sangho/ML/python_ML/LLM2/PJT3/kb운전자보험.pdf\" # \"/content/drive/MyDrive/kb운전자보험.pdf\"\n",
    "vectorstore_name = \"Korean_PDF_HuggingFace_Embeddings\"\n",
    "\n",
    "# PDF 로드 및 청크 생성\n",
    "chunks = load_and_split_pdf(path_ins)\n",
    "\n",
    "# 벡터 스토어 생성 및 저장\n",
    "vector_store = process_pdf_to_vectorstore(vectorstore_name, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIMc2zOY1jbp"
   },
   "source": [
    "# Chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "46r9Nn_t1kJe"
   },
   "outputs": [],
   "source": [
    "# 벡터 스토어 불러오기\n",
    "def load_vectorstore(vectorstore_name):\n",
    "    return Chroma(persist_directory=f\"./data/vector_stores/{vectorstore_name}\")\n",
    "\n",
    "# 리트리버 생성\n",
    "def create_retriever(vector_store):\n",
    "    return vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # MMR: 내용의 중복을 줄이고 다양성을 제공, Similarity: 내용의 유사도를 기준으로 내요을 검색\n",
    "\n",
    "\n",
    "# LLM 인스턴스 생성\n",
    "def instantiate_LLM():\n",
    "    # HuggingFaceHub 모델을 사용할 경우 아래와 같이 설정\n",
    "    return HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\": 0.8})\n",
    "\n",
    "# 대화형 리트리버 체인 생성\n",
    "def create_conversational_chain(llm, retriever):\n",
    "    # ConversationalRetrievalChain 생성\n",
    "    return ConversationalRetrievalChain.from_llm(llm, retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXef0V1I8gG6",
    "outputId": "87ed91a0-86b2-4919-a00a-429a811cb97f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74069/539021787.py:9: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "# 환경 변수로 OPENAI_API_KEY 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "# 벡터 스토어에서 리트리버 생성\n",
    "retriever = create_retriever(vector_store)\n",
    "\n",
    "\n",
    "# LLM 인스턴스 생성, OpenAI 모델 초기화\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-4o-mini',\n",
    "    streaming=False,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    temperature=0.5  # Adjust temperature for response creativity\n",
    ")\n",
    "\n",
    "\n",
    "# 대화형 리트리버 체인 생성\n",
    "conversation_chain = create_conversational_chain(llm, retriever=retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBkShRE01lrj"
   },
   "source": [
    "# 사용자 질문 처리 함수(프롬프트 엔지니어링 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C85aNqDa4Avi"
   },
   "outputs": [],
   "source": [
    "def generate_conversation_prompt(question, chat_history):\n",
    "    \"\"\"\n",
    "    대답을 하는 요령은 다음과 같이 7가지가 있으니 이를 고려하여 대답해줘.\n",
    "    1. 이전 대화를 고려하여 대답한다.\n",
    "    2. 만약 단어와 같이 짧은 입력을 받을 경우 임의로 문장을 완성해서 대답한다. 만약 입력받은 query가 '사과'일 경우 \"사과의 종류는 '맥시토신', '후지', '갤릭' 등이 있습니다.\"와 같은 문장으로 변환한다. 이후 변환된 query에 맞는 답을 생성한다.\n",
    "    3. prompt = \"보험 약관에서 다음 조건에 대한 정보를 제공해주세요:\n",
    "      - 보험금 청구 절차\n",
    "      - 보장 범위\n",
    "      - 면책 사항\n",
    "      - 계약 해지 규정\"\n",
    "    4. prompt = \"다음 질문에 대한 답변을 '보험 약관'에서 찾아주세요. 보험금 청구를 위한 필요한 서류와 절차는 무엇인가요?\"\n",
    "    5. prompt = \"보험 약관에 대하여 질문한다면 '보험 약관'과 관련된 조건을 찾고, 이를 간략하게 요약해 주세요.\"\n",
    "    5. prompt = \"보험 약관에 대하여 질문한다면 '보험금 지급'과 관련된 조건을 찾고, 이를 간략하게 요약해 주세요.\"\n",
    "    6. prompt = \"보험 약관의 내용을 검토하고, 보장 범위나 면책 사항에 대해 모호하거나 애매한 부분을 찾아 알려 주세요.\"\n",
    "    7. prompt = \"보험 약관에 정의된 주요 용어들, 예를 들어 '보험금', '면책', '보장' 등을 각각 정의해주세요.\"\n",
    "    \"\"\"\n",
    "\n",
    "    # 이전 대화 내용이 있을 때, 이를 반영하여 자연스러운 답변을 유도\n",
    "    if chat_history:\n",
    "        # 마지막 질문과 답변을 포함하여 답변을 생성\n",
    "        last_question, last_answer = chat_history[-1]\n",
    "        return f\"이전 대화 내용을 고려하여, '{last_question}'에 대한 답변 '{last_answer}'을 바탕으로 '{question}'에 대해 대답해 주세요.\"\n",
    "    else:\n",
    "        return f\"'{question}'에 대해 대답해 주세요.\"\n",
    "\n",
    "\n",
    "def generate_search_prompt(question):\n",
    "    \"\"\"\n",
    "    정보 검색을 위한 프롬프트 생성 함수\n",
    "    - 주어진 질문에 대해 정보를 제공하도록 유도\n",
    "    \"\"\"\n",
    "    return f\"다음 질문에 대한 정보를 제공해 주세요: {question}\"\n",
    "# 대화 처리 함수\n",
    "def ask_question(question, chat_history):\n",
    "    \"\"\"\n",
    "    두 체인을 연결하여 질문에 답변을 생성.\n",
    "    - conversation_chain: 대화의 흐름을 유지\n",
    "    - 네이버 API로 검색하여 관련 정보를 제공\n",
    "    \"\"\"\n",
    "    # 1. 대화형 체인에서 응답 생성\n",
    "    conversation_response = conversation_chain({\"question\": question, \"chat_history\": chat_history})\n",
    "    conversation_answer = conversation_response.get(\"answer\")\n",
    "\n",
    "    # 2. 대화형 체인이 비어 있으면 네이버 API로 검색\n",
    "    if conversation_answer is None:\n",
    "        search_results = naver_blog_search(question)\n",
    "        filtered_results = filter_blog_results(search_results)\n",
    "\n",
    "        # 결과가 있으면 출력\n",
    "        if filtered_results:\n",
    "            final_answer = \"\\n\".join([f\"Title: {item['title']}\\nLink: {item['link']}\\nDescription: {item['description']}\"\n",
    "                                      for item in filtered_results])\n",
    "        else:\n",
    "            final_answer = \"해당 내용을 찾을 수 없습니다.\"\n",
    "    else:\n",
    "        final_answer = conversation_answer  # 대화형 답변이 있으면 그대로 사용\n",
    "\n",
    "    # 3. 대화 내역 업데이트\n",
    "    chat_history.append((question, final_answer))\n",
    "\n",
    "    return final_answer, chat_history\n",
    "\n",
    "\n",
    "# 연속 대화를 위한 예시\n",
    "def run_conversation():\n",
    "    chat_history = []  # 초기 대화 내역\n",
    "    print(\"종료하려면 '종료'를 입력해 주세요.\")\n",
    "    while True:\n",
    "        # 사용자 입력 받기\n",
    "        question = input(\"질문을 입력하세요: \")\n",
    "        if question.lower() == '종료':  # 'exit' 입력 시 대화 종료\n",
    "            break\n",
    "\n",
    "        # 질문과 응답 처리\n",
    "        final_answer, updated_chat_history = ask_question(question, chat_history)\n",
    "\n",
    "        # 결과 출력\n",
    "        print(\"(쳇봇 이름) 응답:\", final_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9JjL8gW1qp9"
   },
   "source": [
    "# 네이버 api를 사용한 데이터 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zX3qkPO_Bk7p"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 네이버 블로그 검색 함수\n",
    "def naver_blog_search(query):\n",
    "    client_id = \"mdSSzD3WTLqpFZLoxOOi\"\n",
    "    client_secret = \"icocXu9OKJ\"\n",
    "    url = \"https://openapi.naver.com/v1/search/blog.json\"\n",
    "\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"display\": 10,\n",
    "        \"start\": 1\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# 검색 결과 필터링\n",
    "def filter_blog_results(search_results):\n",
    "    if not search_results:\n",
    "        return []\n",
    "\n",
    "    filtered_results = []\n",
    "\n",
    "    for item in search_results['items']:\n",
    "        title = item.get('title', '').replace('<b>', '').replace('</b>', '')\n",
    "        link = item.get('link', '')\n",
    "        description = item.get('description', '').replace('<b>', '').replace('</b>', '')\n",
    "\n",
    "        filtered_results.append({\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'description': description\n",
    "        })\n",
    "\n",
    "    return filtered_results\n",
    "\n",
    "# PDF 로드 및 청크 생성 (기존 PDF 처리 함수)\n",
    "def load_and_split_pdf(path_ins):\n",
    "    loader = PyPDFLoader(path_ins)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 텍스트 분할기 사용하여 텍스트를 청크로 나누기\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators = ['\\n\\n', '\\n', ' ', ''],\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "# 네이버 블로그 검색 결과를 청크로 변환하는 함수\n",
    "def create_chunks_from_blog_results(filtered_results):\n",
    "    chunks = []\n",
    "    for result in filtered_results:\n",
    "        text = f\"Title: {result['title']}\\nLink: {result['link']}\\nDescription: {result['description']}\"\n",
    "        chunks.append(text)\n",
    "    return chunks\n",
    "\n",
    "# 벡터스토어 생성 및 저장\n",
    "def process_pdf_to_vectorstore(vectorstore_name, chunks):\n",
    "    # 임베딩 생성\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
    "\n",
    "    # 벡터스토어 생성 (Chroma 사용)\n",
    "    vector_store = Chroma.from_documents(chunks, embeddings)\n",
    "\n",
    "    # 벡터 스토어 저장\n",
    "    vector_store.persist()\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "# 네이버 블로그 결과를 벡터스토어에 추가하는 함수\n",
    "def add_blog_results_to_vectorstore(query, vectorstore_name, vector_store):\n",
    "    search_results = naver_blog_search(query)\n",
    "    filtered_results = filter_blog_results(search_results)\n",
    "\n",
    "    # 블로그 결과를 청크로 변환\n",
    "    chunks = create_chunks_from_blog_results(filtered_results)\n",
    "\n",
    "    # 새 데이터를 기존 벡터스토어에 추가\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
    "    vector_store.add_texts(texts = chunks, embedding = embeddings)\n",
    "\n",
    "    # 벡터 스토어 저장\n",
    "    vector_store.persist()\n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoF6tPX_GfNI",
    "outputId": "4c037ec1-b8a9-49b9-cffc-2db8a3805047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "종료하려면 '종료'를 입력해 주세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74069/2935729549.py:41: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  conversation_response = conversation_chain({\"question\": question, \"chat_history\": chat_history})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(쳇봇 이름) 응답: 중상해란 사람의 신체를 상해하여 생명에 대한 위험을 발생하게 하거나, 신체의 상해로 인해 불구 또는 불치나 난치의 질병에 이르게 한 경우를 말합니다.\n",
      "(쳇봇 이름) 응답: 보험금 지급 사유는 다음과 같습니다:\n",
      "\n",
      "1. 상해 사망 및 고도후유장해 특별약관:\n",
      "   - 보험기간 중에 상해의 직접결과로 사망한 경우: 사망보험금\n",
      "   - 상해로 인해 고도후유장해상태가 되었을 때: 고도후유장해보험금\n",
      "\n",
      "2. 신주말교통 상해위험 특별약관:\n",
      "   - 신주말교통사고로 인한 상해의 직접결과로 사망한 경우: 사망보험금\n",
      "   - 신주말교통사고로 인한 장해상태가 되었을 때: 후유장해보험금\n",
      "\n",
      "3. 탈구, 신경손상, 압착손상진단위로금 특별약관:\n",
      "   - 상해의 직접결과로 탈구, 신경손상, 압착손상으로 진단확정된 경우: 보험가입금액을 지급\n",
      "\n",
      "각 특별약관에 따라 지급 사유가 다를 수 있으며, 구체적인 조건은 약관에 명시되어 있습니다.\n",
      "(쳇봇 이름) 응답: 접촉 사고 시 보험금 지급 사유는 다음과 같습니다:\n",
      "\n",
      "1. 피보험자가 자동차를 운전하던 중에 발생한 급격하고도 우연한 자동차사고로 신체에 상해를 입은 경우.\n",
      "2. 피보험자가 운행 중인 자동차에 탑승하고 있지 않은 상태에서 발생한 급격하고도 우연한 외래의 사고로 신체에 상해를 입은 경우.\n",
      "3. 피보험자가 운행 중인 자동차에 탑승하지 않은 때, 운행 중인 자동차와의 충돌, 접촉 또는 이들 자동차의 충돌, 접촉, 화재 또는 폭발 등의 사고로 신체에 상해를 입은 경우.\n",
      "\n",
      "이러한 경우에는 보험가입금액이 보험수익자에게 지급됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 대화 시작\n",
    "run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KmX2b_JgRRFn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pro3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
