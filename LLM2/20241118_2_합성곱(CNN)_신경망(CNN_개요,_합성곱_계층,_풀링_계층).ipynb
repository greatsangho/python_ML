{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76VlQtv3cR9r",
    "outputId": "8ec72824-30f7-4d47-eaa1-f24dfb856038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8adhrVnabXDP",
    "outputId": "d8b7d955-5daa-4f9f-aabd-e1d1afc2b120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/sangho/.cache/kagglehub/datasets/fournierp/captcha-version-2-images/versions/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W dml_heap_allocator.cc:93] DML allocator out of memory!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unknown error -2147024882",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 196\u001b[0m\n\u001b[1;32m    191\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Captcha(\n\u001b[1;32m    192\u001b[0m     pth\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sangho/.cache/kagglehub/datasets/fournierp/captcha-version-2-images/versions/2/samples/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCRNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBOW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m optim \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unknown error -2147024882"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"fournierp/captcha-version-2-images\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "from glob import glob\n",
    "imeages = glob('/home/sangho/.cache/kagglehub/datasets/fournierp/captcha-version-2-images/versions/2/**/*.png',recursive=True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# 문자들로부터 BOW를 만드는 함수\n",
    "def get_BOW(corpus):\n",
    "   # 공백문자 <pad>를 0으로 설정\n",
    "   BOW = {\"<pad>\":0}\n",
    "\n",
    "   # corpus의 문자들을 이용해 BOW에 고유번호 추가\n",
    "   for letter in corpus:\n",
    "       if letter not in BOW.keys():\n",
    "           BOW[letter] = len(BOW.keys())\n",
    "\n",
    "   return BOW\n",
    "\n",
    "\n",
    "class Captcha(Dataset):\n",
    "   def __init__(self, pth, train=True):\n",
    "       # 소문자와 숫자만 정답으로 이용\n",
    "       self.corpus = string.ascii_lowercase + string.digits\n",
    "       self.BOW = get_BOW(self.corpus)\n",
    "\n",
    "       # 불러올 이미지 파일의 경로\n",
    "       self.imgfiles = glob.glob(pth+\"/*.png\")\n",
    "\n",
    "       self.train = train\n",
    "       self.trainset = self.imgfiles[:int(len(self.imgfiles)*0.8)]\n",
    "       self.testset = self.imgfiles[int(len(self.imgfiles)*0.8):]\n",
    "\n",
    "   # 문자와 숫자를 고유번호로 치환\n",
    "   def get_seq(self, line):\n",
    "       label = []\n",
    "\n",
    "       for letter in line:\n",
    "           label.append(self.BOW[letter])\n",
    "\n",
    "       return label\n",
    "\n",
    "   def __len__(self):\n",
    "       if self.train:\n",
    "           return len(self.trainset)\n",
    "       else:\n",
    "           return len(self.testset)\n",
    "\n",
    "   def __getitem__(self, i):\n",
    "       if self.train:\n",
    "           # png파일을 RGB파일로 변환\n",
    "           data = Image.open(self.trainset[i]).convert(\"RGB\")\n",
    "\n",
    "           label = self.trainset[i].split(\"/\")[-1]\n",
    "           # 파일이름에서 확장자를 제거\n",
    "           label = label.split(\".png\")[0]\n",
    "           # 정답 문자열을 BOW의 순열로 변환\n",
    "           label = self.get_seq(label)\n",
    "\n",
    "           data = np.array(data).astype(np.float32)\n",
    "           # 파이토치는 채널이 가장 앞에 와야 함\n",
    "           data = np.transpose(data, (2, 0, 1))\n",
    "           label = np.array(label)\n",
    "\n",
    "           return data, label\n",
    "\n",
    "       else:\n",
    "           data = Image.open(self.testset[i]).convert(\"RGB\")\n",
    "           label = self.testset[i].split(\"/\")[-1]\n",
    "           label = label.split(\".png\")[0]\n",
    "           label = self.get_seq(label)\n",
    "\n",
    "           data = np.array(data).astype(np.float32)\n",
    "           label = np.array(label)\n",
    "\n",
    "           return data, label\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 3X5 크기의 커널을 이용\n",
    "class BasicBlock(nn.Module):\n",
    "   def __init__(self,\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(3, 5),\n",
    "                stride=(2, 1)):\n",
    "       super(BasicBlock, self).__init__()\n",
    "\n",
    "       self.c1 = nn.Conv2d(in_channels=in_channels,\n",
    "                           out_channels=out_channels,\n",
    "                           kernel_size=kernel_size,\n",
    "                           stride=stride)\n",
    "       self.c2 = nn.Conv2d(in_channels=out_channels,\n",
    "                           out_channels=out_channels,\n",
    "                           kernel_size=(3, 3), padding=1)\n",
    "\n",
    "       self.downsample = nn.Conv2d(in_channels=in_channels,\n",
    "                                   out_channels=out_channels,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=stride)\n",
    "\n",
    "       self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "       self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "       self.relu = nn.ReLU()\n",
    "\n",
    "   def forward(self, x):\n",
    "       x_ = x\n",
    "\n",
    "       x = self.c1(x)\n",
    "       x = self.bn1(x)\n",
    "       x = self.relu(x)\n",
    "       x = self.c2(x)\n",
    "       x = self.bn2(x)\n",
    "\n",
    "       x_ = self.downsample(x_)\n",
    "\n",
    "       x += x_\n",
    "       x = self.relu(x)\n",
    "\n",
    "       return x\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "   def __init__(self, output_size):\n",
    "       super(CRNN, self).__init__()\n",
    "\n",
    "       # CNN층의 정의\n",
    "       self.c1 = BasicBlock(in_channels=3, out_channels=64)\n",
    "       self.c2 = BasicBlock(in_channels=64, out_channels=64)\n",
    "       self.c3 = BasicBlock(in_channels=64, out_channels=64)\n",
    "       self.c4 = BasicBlock(in_channels=64, out_channels=64)\n",
    "       self.c5 = nn.Conv2d(64, 64, kernel_size=(2, 5))\n",
    "\n",
    "       # 텍스트 정보를 추출할 GRU층\n",
    "       self.gru = nn.GRU(64, 64, batch_first=False)\n",
    "\n",
    "       # 분류를 위한 MLP층\n",
    "       self.fc1 = nn.Linear(in_features=64, out_features=128)\n",
    "       self.fc2 = nn.Linear(in_features=128, out_features=output_size)\n",
    "       self.relu = nn.ReLU()\n",
    "\n",
    "   def forward(self, x):\n",
    "       # 입력텐서의 모양(B, 3, 50, 200)\n",
    "       x = self.c1(x)\n",
    "       x = self.c2(x)\n",
    "       x = self.c3(x)\n",
    "       x = self.c4(x)\n",
    "       x = self.c5(x)\n",
    "       # 특징 추출 후 텐서의 모양(B, 64, 1, 180)\n",
    "\n",
    "       # (B, 64, 180)으로 모양을 변경\n",
    "       x = x.view(x.shape[0], 64, -1)\n",
    "       # (B, 180, 64)로 모양을 변경\n",
    "       x = x.permute(2, 0, 1)\n",
    "\n",
    "       # GRU로 시계열 정보 추출\n",
    "       x, _ = self.gru(x)\n",
    "\n",
    "       # FC층으로 각 픽셀 분류\n",
    "       x = self.fc1(x)\n",
    "       x = self.relu(x)\n",
    "       x = self.fc2(x)\n",
    "\n",
    "       #CTC 손실 계산을 위해 로그 소프트맥스를 이용\n",
    "       x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "       return x\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "import torch_directml\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch_directml.device()\n",
    "\n",
    "dataset = Captcha(\n",
    "    pth=\"/home/sangho/.cache/kagglehub/datasets/fournierp/captcha-version-2-images/versions/2/samples/\"\n",
    "    )\n",
    "loader = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "model = CRNN(output_size=len(dataset.BOW)).to(device)\n",
    "\n",
    "optim = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(200):\n",
    "   iterator = tqdm.tqdm(loader)\n",
    "\n",
    "   # ❷ 정답에 사용할 label\n",
    "   for data, label in iterator:\n",
    "       optim.zero_grad()\n",
    "       preds = model(data.to(device))  #  CRNN의 출력값\n",
    "\n",
    "       # CTC 손실 계산은 텐서의 모양까지 넣어줘야 함\n",
    "       # 정수형으로 간단하게 preds와 label의 모양을 만들어주자\n",
    "\n",
    "       # 시계열을 묶은 모양을 나타내는 변수\n",
    "       preds_size = torch.IntTensor([preds.size(0)] * 8).to(device)\n",
    "       # 정답의 모양을 나타내는 변수\n",
    "       target_len = torch.IntTensor([len(txt) for txt in label]).to(device)\n",
    "\n",
    "       # 손실 계산\n",
    "       loss = nn.CTCLoss(blank=0)(\n",
    "           preds, label.to(device), preds_size, target_len)\n",
    "\n",
    "       loss.backward()   # 역전파\n",
    "       optim.step()\n",
    "\n",
    "       iterator.set_description(f\"epoch{epoch+1} loss:{loss.item()}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"//model/CRNN2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
