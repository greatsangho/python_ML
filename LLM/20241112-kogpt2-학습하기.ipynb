{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 훈련 데이터 크기: (145140, 3)\n",
      "전처리 후 테스트 데이터 크기: (48822, 3)\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "# 데이터 불러오기\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "# 결측치 제거\n",
    "train_data.dropna(inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "\n",
    "# 한글만 추출 (정규식 사용)\n",
    "train_data['document'] = train_data['document'].apply(lambda x: re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\s]', ' ', str(x)))\n",
    "test_data['document'] = test_data['document'].apply(lambda x: re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\s]', ' ', str(x)))\n",
    "\n",
    "# 중복 제거\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "\n",
    "# 인덱스 리셋\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f'전처리 후 훈련 데이터 크기: {train_data.shape}')\n",
    "print(f'전처리 후 테스트 데이터 크기: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    145140.000000\n",
      "mean          8.023942\n",
      "std           6.689301\n",
      "min           0.000000\n",
      "25%           4.000000\n",
      "50%           6.000000\n",
      "75%          10.000000\n",
      "max          47.000000\n",
      "Name: document, dtype: float64\n",
      "max_token_len : 24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_data['document'].map(lambda x: len(x.split())).describe())\n",
    "import numpy as np\n",
    "max_token_len = np.percentile(train_data['document'].map(lambda x: len(x.split())).to_numpy(), 95)\n",
    "max_token_len = int(max_token_len)\n",
    "print(f'max_token_len : {max_token_len}')\n",
    "train_data['document'].apply(lambda x: len(x.split())).hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/sangho/miniconda3/envs/ml2/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/9072 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The GPU device does not support Double (Float64) operations!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m [b\u001b[38;5;241m.\u001b[39mto(dml_device) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch]  \u001b[38;5;66;03m# 모든 텐서를 DirectML 장치로 이동\u001b[39;00m\n\u001b[1;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 58\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)  \u001b[38;5;66;03m# CrossEntropyLoss 사용\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1585\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1583\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1585\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1598\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1599\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1040\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _use_sdpa:\n\u001b[0;32m-> 1040\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_causal_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;66;03m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;66;03m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;66;03m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m         \u001b[38;5;66;03m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m         \u001b[38;5;66;03m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:391\u001b[0m, in \u001b[0;36m_prepare_4d_causal_attention_mask_for_sdpa\u001b[0;34m(attention_mask, input_shape, inputs_embeds, past_key_values_length, sliding_window)\u001b[0m\n\u001b[1;32m    389\u001b[0m     expanded_4d_mask \u001b[38;5;241m=\u001b[39m attention_mask\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     expanded_4d_mask \u001b[38;5;241m=\u001b[39m \u001b[43mattn_mask_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_4d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# Attend to all tokens in masked rows from the causal_mask, for example the relevant first rows when\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# Details: https://github.com/pytorch/pytorch/issues/110213\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m expanded_4d_mask\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:123\u001b[0m, in \u001b[0;36mAttentionMaskConverter.to_4d\u001b[0;34m(self, attention_mask_2d, query_length, dtype, key_value_length)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis attention mask converter is causal. Make sure to pass `key_value_length` to correctly create a causal mask.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         )\n\u001b[1;32m    122\u001b[0m     past_key_values_length \u001b[38;5;241m=\u001b[39m key_value_length \u001b[38;5;241m-\u001b[39m query_length\n\u001b[0;32m--> 123\u001b[0m     causal_4d_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask_2d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSliding window is currently only implemented for causal masking\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:158\u001b[0m, in \u001b[0;36mAttentionMaskConverter._make_causal_mask\u001b[0;34m(input_ids_shape, dtype, device, past_key_values_length, sliding_window)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03mMake causal mask used for bi-directional self-attention.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m bsz, tgt_len \u001b[38;5;241m=\u001b[39m input_ids_shape\n\u001b[0;32m--> 158\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m mask_cond \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    160\u001b[0m mask\u001b[38;5;241m.\u001b[39mmasked_fill_(mask_cond \u001b[38;5;241m<\u001b[39m (mask_cond \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The GPU device does not support Double (Float64) operations!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "from transformers import PreTrainedTokenizerFast, GPT2ForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "torch.set_default_dtype(torch.float16)\n",
    "# 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', pad_token='<pad>')\n",
    "\n",
    "# KoGPT2 모델 로드 (이진 분류를 위한 GPT2ForSequenceClassification 사용)\n",
    "model = GPT2ForSequenceClassification.from_pretrained('skt/kogpt2-base-v2', num_labels=2)  # num_labels=2로 설정\n",
    "# DirectML 장치 설정\n",
    "dml_device = torch_directml.device()  # DirectML 장치 설정\n",
    "model.half().to(dml_device)  # 모델을 DirectML 장치로 이동\n",
    "\n",
    "# 데이터 타입을 명확히 지정한 tokenize_and_encode 함수\n",
    "def tokenize_and_encode(data, tokenizer):\n",
    "    encodings = tokenizer(\n",
    "        data['document'].tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=128,\n",
    "    )\n",
    "    return encodings\n",
    "\n",
    "# train_data와 test_data의 문자열 데이터를 정수형 텐서로 변환 (토크나이저 사용)\n",
    "train_encodings = tokenize_and_encode(train_data, tokenizer)\n",
    "test_encodings = tokenize_and_encode(test_data, tokenizer)\n",
    "\n",
    "# 레이블은 이진 분류 작업이므로 int64로 변환 (CrossEntropyLoss를 사용할 경우)\n",
    "train_labels = torch.tensor(train_data['label'].values, dtype=torch.int64)  # 정수형 레이블\n",
    "test_labels = torch.tensor(test_data['label'].values, dtype=torch.int64)    # 정수형 레이블\n",
    "\n",
    "# TensorDataset 및 DataLoader 생성 (배치 크기 줄임)\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Optimizer 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 손실 함수 설정 (CrossEntropyLoss 사용)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 학습 루프 정의\n",
    "for epoch in range(1):  # 에포크 수 설정\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids, attention_mask, labels = [b.to(dml_device) for b in batch]  # 모든 텐서를 DirectML 장치로 이동\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss = criterion(logits, labels)  # CrossEntropyLoss 사용\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1} completed.')\n",
    "\n",
    "# 모델 평가\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(dml_device) for b in batch]  # 모든 텐서를 DirectML 장치로 이동\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 추론 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "/home/sangho/miniconda3/envs/ml2/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The GPU device does not support Double (Float64) operations!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     90\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 91\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     93\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1271\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1271\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1040\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _use_sdpa:\n\u001b[0;32m-> 1040\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_causal_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;66;03m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;66;03m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;66;03m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m         \u001b[38;5;66;03m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m         \u001b[38;5;66;03m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:391\u001b[0m, in \u001b[0;36m_prepare_4d_causal_attention_mask_for_sdpa\u001b[0;34m(attention_mask, input_shape, inputs_embeds, past_key_values_length, sliding_window)\u001b[0m\n\u001b[1;32m    389\u001b[0m     expanded_4d_mask \u001b[38;5;241m=\u001b[39m attention_mask\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     expanded_4d_mask \u001b[38;5;241m=\u001b[39m \u001b[43mattn_mask_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_4d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# Attend to all tokens in masked rows from the causal_mask, for example the relevant first rows when\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# Details: https://github.com/pytorch/pytorch/issues/110213\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m expanded_4d_mask\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:123\u001b[0m, in \u001b[0;36mAttentionMaskConverter.to_4d\u001b[0;34m(self, attention_mask_2d, query_length, dtype, key_value_length)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis attention mask converter is causal. Make sure to pass `key_value_length` to correctly create a causal mask.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         )\n\u001b[1;32m    122\u001b[0m     past_key_values_length \u001b[38;5;241m=\u001b[39m key_value_length \u001b[38;5;241m-\u001b[39m query_length\n\u001b[0;32m--> 123\u001b[0m     causal_4d_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask_2d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSliding window is currently only implemented for causal masking\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:158\u001b[0m, in \u001b[0;36mAttentionMaskConverter._make_causal_mask\u001b[0;34m(input_ids_shape, dtype, device, past_key_values_length, sliding_window)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03mMake causal mask used for bi-directional self-attention.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m bsz, tgt_len \u001b[38;5;241m=\u001b[39m input_ids_shape\n\u001b[0;32m--> 158\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m mask_cond \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    160\u001b[0m mask\u001b[38;5;241m.\u001b[39mmasked_fill_(mask_cond \u001b[38;5;241m<\u001b[39m (mask_cond \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The GPU device does not support Double (Float64) operations!"
     ]
    }
   ],
   "source": [
    "# 데이터 셋을 토큰화, 토크나이져 로드, 모델로드\n",
    "model_name = 'skt/kogpt2-base-v2'\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>')\n",
    "\n",
    "# 대화형식/쳇봇\n",
    "# 질문 답변형식으로 데이터를 인코딩\n",
    "\n",
    "# question = '안녕하세요! 오늘 날씨가 어때요?'\n",
    "# answer = '안녕하세요! 오늘 날씨는 맑고 화창합니다.'\n",
    "\n",
    "# input_text = f'질문:{question} 답변:'\n",
    "# target_text = f\"{answer}\"\n",
    "# input_ids =  tokenizer.encode(input_text,add_special_tokens=True, truncation=True, max_length=128)\n",
    "# target_ids =  tokenizer.encode(target_text,add_special_tokens=True, truncation=True, max_length=128)\n",
    "# target_ids\n",
    "\n",
    "\n",
    "# 모델 불러오기\n",
    "# 데이터  로더 사용\n",
    "import torch\n",
    "import torch_directml\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.set_default_dtype(torch.float16)\n",
    "class QADataSet(Dataset):\n",
    "  def __init__(self,tokenizer,question,answer,max_length = 128):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.tokenizer.pad_token = tokenizer.eos_token\n",
    "    self.question = question\n",
    "    self.answer = answer\n",
    "    self.max_length = max_length\n",
    "    self.input_ids = []\n",
    "    self.labels = []\n",
    "    self.attention_mask = []\n",
    "    for question, answer in zip(question,answer):\n",
    "      input_text = f'질문:{question} 답변:'\n",
    "      target_text = f\"{answer}\"\n",
    "      input_encodings = tokenizer(input_text, padding='max_length', add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "      target_encodings = tokenizer(target_text,padding='max_length',add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "\n",
    "      self.input_ids.append(input_encodings['input_ids'])\n",
    "      self.labels.append(target_encodings['input_ids'])\n",
    "      self.attention_mask.append(input_encodings['attention_mask'])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.question)\n",
    "  def __getitem__(self,idx):\n",
    "    question = self.question[idx]\n",
    "    answer = self.answer[idx]\n",
    "    return {\n",
    "      'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
    "      'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
    "      'labels' : torch.tensor(self.labels[idx],dtype=torch.long)\n",
    "    }\n",
    "questions = ['안녕하세요 오늘날씨가 어때요?','단위프로젝트 주제는 뭔가요?','프로젝트 목표는 뭔가요?']\n",
    "answers = ['안녕하세요 오늘 날씨는 좋아요','내외부문서를 활용한 QA 시스템 입니다.',\n",
    "           '환각을 방지하고 원하는 내외부 데이터 내에서 RAG 기반 LLM 활용 질의 응답 시스템 구현\\\n",
    "           ,문서를 벡터 형태로 임베딩하여 벡터데이터 ']\n",
    "\n",
    "\n",
    "dataset = QADataSet(tokenizer, question=questions,answer=answers)\n",
    "dataloader = DataLoader(dataset,batch_size=2)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch_directml.device()\n",
    "\n",
    "# 모델 로드\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "# 옵티마이져\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(),lr=1e-5)\n",
    "model.to(device)\n",
    "# 스케줄러 적용\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "epochs = 10\n",
    "total_steps = len(dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
    "\n",
    "model.train()\n",
    "\n",
    "from tqdm import tqdm\n",
    "iterator = tqdm(range(epochs))\n",
    "for epoch in iterator:\n",
    "  for batch in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids,attention_mask=attention_mask,labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    iterator.set_description(f'Epoch {epoch+1} loss :{loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
