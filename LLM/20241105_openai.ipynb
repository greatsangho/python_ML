{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "# os.environ['OPENAI_API_KEY'] = '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'\n",
    "os.environ.get('OpenAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# openai.api_key = ''\n",
    "response = openai.chat.completions.create(\n",
    "    # model= \"gpt-3.5-turbo\",\n",
    "    model= 'gpt-4o-mini',\n",
    "    messages = [\n",
    "        # {'role':'system', 'content':'You are a helpful assistant'},\n",
    "        # {'role':'user', 'content':'Who won the world series in 2020'},\n",
    "        # {'role':'assistant','content':'The Los Angeles Dodgers won the World Series in 2020'},\n",
    "        # {'role':'user', 'content':'Where was it played?'},\n",
    "        {'role':'user', 'content':'ì ì‹¬ ë©”ë‰´ë¥¼ ì¶”ì²œí•´ì¤˜'}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì ì‹¬ ë©”ë‰´ë¡œ ëª‡ ê°€ì§€ ì¶”ì²œë“œë¦´ê²Œìš”!\\n\\n1. **ë¹„ë¹”ë°¥**: ì—¬ëŸ¬ ê°€ì§€ ì±„ì†Œì™€ ê³ ê¸°ë¥¼ ì„ì–´ì„œ ê³ ì¶”ì¥ì„ ê³ë“¤ì—¬ ë¨¹ëŠ” ê±´ê°•í•œ í•œ ë¼ì…ë‹ˆë‹¤.\\n2. **ê¹€ì¹˜ì°Œê°œ**: ë”°ëœ»í•˜ê³  ì–¼í°í•œ ê¹€ì¹˜ì°Œê°œì— ë°¥ê³¼ í•¨ê»˜ ì¦ê¸°ë©´ ì¢‹ìŠµë‹ˆë‹¤.\\n3. **ìƒŒë“œìœ„ì¹˜**: ê°„í¸í•˜ê²Œ ë¨¹ì„ ìˆ˜ ìˆëŠ” ìƒŒë“œìœ„ì¹˜ì— ì‹ ì„ í•œ ì±„ì†Œì™€ ê³ ê¸° ë˜ëŠ” ì¹˜ì¦ˆë¥¼ ë„£ì–´ë³´ì„¸ìš”.\\n4. **ë‹­ê°ˆë¹„**: ë§¤ì½¤í•œ ì–‘ë…ìœ¼ë¡œ ë³¶ì€ ë‹­ê°ˆë¹„ëŠ” ë°¥ê³¼ í•¨ê»˜ ë¨¹ê¸° ì¢‹ì€ ë©”ë‰´ì…ë‹ˆë‹¤.\\n5. **íŒŒìŠ¤íƒ€**: í¬ë¦¼, í† ë§ˆí† , ì˜¤ì¼ ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ì™€ í•¨ê»˜ ì¦ê¸¸ ìˆ˜ ìˆëŠ” íŒŒìŠ¤íƒ€ë„ ì¢‹ì€ ì„ íƒì…ë‹ˆë‹¤.\\n\\nì–´ë–¤ ë©”ë‰´ê°€ ë§ˆìŒì— ë“œì‹œë‚˜ìš”?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5001\n",
      " * Running on http://172.21.70.32:5001\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# flaskë¥¼ ì´ìš©í•´ì„œ \n",
    "import openai\n",
    "from flask import Flask, jsonify, request\n",
    "import os\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/prompt', methods = ['POST'])\n",
    "def generate_answer():    \n",
    "    if request.method == 'POST':\n",
    "        prompt = request.json['prompt']\n",
    "        openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "        pre_prompt = 'í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì¤˜\\n\\n'\n",
    "        response = openai.chat.completions.create(\n",
    "            model = \"gpt-3.5-turbo\",  # ë¬´ë£Œ\n",
    "            # model = \"gpt-4o\",  # ìœ ë¡œ\n",
    "            messages = [\n",
    "                {'role':'system','content':'You ar a helpful assistant'},\n",
    "                {'role':'user','content':pre_prompt + prompt},                \n",
    "            ],\n",
    "            max_tokens = 3000,\n",
    "            stop = None,\n",
    "            temperature = 0.5  # ì¶œë ¥ì˜ ë‹¤ì–‘ì„±  0.2 ë§¤ìš° ë³´ìˆ˜ì ì´ê³  ì¼ê´€ì„±ì„ìœ ì§€: ê°„ê²°í•œ ë‹µë³€,  0.8: ë§¤ìš° ì°½ì˜ì ì¸ ë‹µë³€\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return jsonify({'answer' : answer})\n",
    "\n",
    "app.run(host=\"0.0.0.0\", debug=False, port=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdall-e-3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1024x1024\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m image_url \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39murl\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_url)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4/lib/python3.9/site-packages/openai/resources/images.py:264\u001b[0m, in \u001b[0;36mImages.generate\u001b[0;34m(self, prompt, model, n, quality, response_format, size, style, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    222\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ImagesResponse:\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    Creates an image given a prompt.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/images/generations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_generate_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageGenerateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mImagesResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4/lib/python3.9/site-packages/openai/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m     )\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4/lib/python3.9/site-packages/openai/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4/lib/python3.9/site-packages/openai/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "# image api\n",
    "# text prompt image DALL.E 3 / DALL.E 2\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "prompt = input(\"Prompt: \")\n",
    "response = client.images.generate(\n",
    "    model= \"dall-e-3\",\n",
    "    prompt=prompt,\n",
    "    size='1024x1024',\n",
    "    quality='hd',\n",
    "    n=1\n",
    ")\n",
    "image_url = response.data[0].url\n",
    "print(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n",
    "# 'Let's think step by step' ì •ë‹µë¥ ì´ 2ë°° ì´ìƒ ìƒìŠ¹\n",
    "# Multi-turn\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ê·¼ìœ¡ì„ ë§Œë“¤ê¸° ìœ„í•œ ì ì‹¬ ë©”ë‰´ë¡œëŠ” ë‹¨ë°±ì§ˆê³¼ ì˜ì–‘ì†Œê°€ í’ë¶€í•œ ìŒì‹ì„ ì¶”ì²œí•©ë‹ˆë‹¤. ë‹¤ìŒì€ ëª‡ ê°€ì§€ ì˜ˆì‹œì…ë‹ˆë‹¤:\\n\\n1. **ë‹­ ê°€ìŠ´ì‚´ ìƒëŸ¬ë“œ**  \\n   - ì¬ë£Œ: ë‹­ ê°€ìŠ´ì‚´, ë‹¤ì–‘í•œ ì±„ì†Œ (ìƒì¶”, ì‹œê¸ˆì¹˜, ë°©ìš¸í† ë§ˆí† , ì˜¤ì´ ë“±), ì•„ë³´ì¹´ë„, ê²¬ê³¼ë¥˜, ì˜¬ë¦¬ë¸Œ ì˜¤ì¼, ë ˆëª¬ì¦™\\n   - ë°©ë²•: ë‹­ ê°€ìŠ´ì‚´ì„ êµ½ê³  ì±„ì†Œì™€ í•¨ê»˜ ì„ì–´ ìƒëŸ¬ë“œë¥¼ ë§Œë“  í›„, ì˜¬ë¦¬ë¸Œ ì˜¤ì¼ê³¼ ë ˆëª¬ì¦™ìœ¼ë¡œ ë“œë ˆì‹±í•©ë‹ˆë‹¤.\\n\\n2. **ê³ ë“±ì–´ êµ¬ì´ì™€ í€´ë…¸ì•„**  \\n   - ì¬ë£Œ: ê³ ë“±ì–´, í€´ë…¸ì•„, ë¸Œë¡œì½œë¦¬, ì˜¬ë¦¬ë¸Œ ì˜¤ì¼, ì†Œê¸ˆ, í›„ì¶”\\n   - ë°©ë²•: ê³ ë“±ì–´ë¥¼ êµ¬ì›Œì„œ í€´ë…¸ì•„ì™€ ì° ë¸Œë¡œì½œë¦¬ì™€ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.\\n\\n3. **í„°í‚¤ ê³ ê¸° ë³´ìš¸**  \\n   - ì¬ë£Œ: ë‹¤ì§„ í„°í‚¤ ê³ ê¸°, í˜„ë¯¸ ë°¥, í”¼ë§, ì–‘íŒŒ, ê°ì, ì•„ë³´ì¹´ë„\\n   - ë°©ë²•: í„°í‚¤ ê³ ê¸°ë¥¼ ë³¶ì•„ ë°¥ê³¼ í•¨ê»˜ ë‹´ê³ , ì±„ì†Œì™€ ì•„ë³´ì¹´ë„ë¥¼ ì˜¬ë ¤ì¤ë‹ˆë‹¤.\\n\\n4. **ë‘ë¶€ ìŠ¤í…Œì´í¬ì™€ ì±„ì†Œ ë³¶ìŒ**  \\n   - ì¬ë£Œ: ë‘ë¶€, ì‹œê¸ˆì¹˜, ë‹¹ê·¼, ë²„ì„¯, ê°„ì¥, ì˜¬ë¦¬ë¸Œ ì˜¤ì¼\\n   - ë°©ë²•: ë‘ë¶€ë¥¼ ë‘íˆ¼í•˜ê²Œ ì°ì–´ êµ¬ìš´ í›„, ì±„ì†Œë¥¼ ë³¶ì•„ì„œ ê³ë“¤ì…ë‹ˆë‹¤.\\n\\nì´ëŸ¬í•œ ë©”ë‰´ëŠ” ë‹¨ë°±ì§ˆì„ ì¶©ë¶„íˆ ì„­ì·¨í•˜ë©´ì„œ ì˜ì–‘ ê· í˜•ë„ ì˜ ë§ì¶œ ìˆ˜ ìˆëŠ” ì ì‹¬ì…ë‹ˆë‹¤. ì ì ˆí•œ ìš´ë™ê³¼ í•¨ê»˜ ì„­ì·¨í•˜ë©´ ê·¼ìœ¡ ë¹Œë”©ì— ë„ì›€ì´ ë©ë‹ˆë‹¤!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "# openai.api_key = ''\n",
    "response = openai.chat.completions.create(\n",
    "    # model= \"gpt-3.5-turbo\",\n",
    "    model= 'gpt-4o-mini',\n",
    "    messages = [\n",
    "        {'role':'system', 'content':'ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” ë‹¹ì‹ ì˜ ì˜ì–‘ì‚¬ì…ë‹ˆë‹¤. ë‹¤ì´ì–´íŠ¸ ë“œì˜¤ê°€ ê°™ì´ ì‹ìŠµê´€ì„ ì•Œë ¤ì£¼ì‹œë©´ ì ì ˆí•œ ë©”ë‰´ì™€ ë ˆì‹œí”¼ë¥¼ ì•Œë ¤ë“œë¦´ê²Œìš”'},\n",
    "        {'role':'user', 'content':'ë‹¤ì´ì–´íŠ¸ ì¤‘ì¸ë° ê±´ê°•ì‹ìœ¼ë¡œ ë¨¹ì„ ì ì‹¬ë©”ë‰´ê°€ ë­ê°€ ìˆì„ê¹Œ?'},\n",
    "        {'role':'assistant','content':'ë‹¨í˜¸ë°• ì£½ì´ ìˆìœ¼ë©° í˜¸ë°•ì„ ì ë‹¹í•˜ê²Œ ìª„ì„œ ìš”ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.'},\n",
    "        {'role':'user', 'content':'ê·¼ìœ¡ì„ ë§Œë“¤ê³  ì‹¶ì€ë° ì ì‹¬ì€ ë­˜ ë¨¹ì„ê¹Œ?'}\n",
    "    ]\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì¸íŠœë‹ - ë¯¸ì„¸ì¡°ì •\n",
    "# few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ê°ì •: ê¸ì •ì '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# few shot\n",
    "import openai\n",
    "prompt = '''\n",
    "ë‹¤ìŒ ë¦¬ë·°ì˜ ê°ì •ì„ íŒë‹¨í•´ ì£¼ì„¸ìš”\n",
    "\n",
    "ë¦¬ë·°: \"ì´ ì˜í™”ëŠ” ì •ë§ ì¬ë¯¸ìˆì–´ìš”! ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤.\"\n",
    "ê°ì •: ê¸ì •ì \n",
    "\n",
    "ë¦¬ë·°: \"ì‹œê°„ ë‚­ë¹„í•œ ê¸°ë¶„ì…ë‹ˆë‹¤.\"\n",
    "ê°ì •: ë¶€ì •ì \n",
    "\n",
    "ë¦¬ë·°: \"ì •ë§ í›Œë¥­í•œ ì—°ê¸°ì™€ ìŠ¤í† ë¦¬\"\n",
    "ê°ì •:\n",
    "'''\n",
    "response = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini-2024-07-18',\n",
    "    messages=[{'role':'user', 'content':prompt}],\n",
    "    max_tokens= 10\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'í•˜ë‚˜ì˜ ìœ ê¸°ì ì¸'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# openaië¡œ í•™ìŠµí•œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ í•´ì„œ ì‚¬ìš©í•˜ê¸°\n",
    "import openai\n",
    "# ft:gpt-3.5-turbo-0125:personal::AQ3Qm0WA\n",
    "prompt = input(\"Prompt : \")\n",
    "response = openai.chat.completions.create(\n",
    "    model='ft:gpt-3.5-turbo-0125:personal::AQ3Qm0WA',\n",
    "    messages=[{'role':'user','content':prompt}],\n",
    "    max_tokens=10\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‰´ìŠ¤ê¸°ì‚¬ ìƒì„±\n",
    "# í•œêµ­ì–´ gpt2 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ, í† í¬ë‚˜ì´ì ¸\n",
    "# gpt2ì— ë§ê²Œ í† í°í™”\n",
    "# ê¸°ì‚¬ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['â–ì•ˆë…•',\n",
       " 'í•˜',\n",
       " 'ì„¸',\n",
       " 'ìš”.',\n",
       " 'â–í•œêµ­ì–´',\n",
       " 'â–G',\n",
       " 'P',\n",
       " 'T',\n",
       " '-2',\n",
       " 'â–ì…',\n",
       " 'ë‹ˆë‹¤.',\n",
       " 'ğŸ˜¤',\n",
       " ':)',\n",
       " 'l^o']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>')\n",
    "tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\")\n",
    "['â–ì•ˆë…•', 'í•˜', 'ì„¸', 'ìš”.', 'â–í•œêµ­ì–´', 'â–G', 'P', 'T', '-2', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ˜¤', ':)', 'l^o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì ì„ ì˜ ìê¸° ìœ„í•´ í•  ìˆ˜ ìˆëŠ”ì¼ ì•Œë ¤ì¤˜ì•¼ í•œë‹¤.\n",
      "ë˜í•œ ìì‹ ì˜ ëª¸ ìƒíƒœë¥¼ ì²´í¬í•´ ë³¼ í•„ìš”ê°€ ìˆë‹¤.\n",
      "ì´ë•ŒëŠ” í‰ì†Œë³´ë‹¤ ë” ë§ì€ ì–‘ì˜ ìŒì‹ì„ ì„­ì·¨í•´ì•¼ í•˜ë©° íŠ¹íˆ ê³¼ì¼ê³¼ ì•¼ì±„ëŠ” ë°˜ë“œì‹œ ì±™ê²¨ ë¨¹ì–´ì•¼ í•œë‹¤.</d> ì§€ë‚œë‹¬ 30ì¼ ì˜¤í›„ ì„œìš¸ ê°•ë‚¨êµ¬ ì‚¼ì„±ë™ ì½”ì—‘ìŠ¤ ì»¨ë²¤ì…˜í™€. â€˜2010 ëŒ€í•œë¯¼êµ­ ëŒ€í‘œë¸Œëœë“œ ëŒ€ìƒâ€™ ì‹œìƒì‹ì´ ì—´ë ¸ë‹¤.\n",
      "ì˜¬í•´ë¡œ 3íšŒì§¸ë¥¼ ë§ì€ ì´ í–‰ì‚¬ëŠ” í•œêµ­ëŠ¥ë¥ í˜‘íšŒì»¨ì„¤íŒ…(KMAC)ì´ ì£¼ìµœí•˜ê³  ì‚°ì—…í†µìƒìì›ë¶€, ì§€ì‹ê²½ì œë¶€ê°€ í›„ì›í•˜ëŠ” êµ­ë‚´ ìµœëŒ€ ê·œëª¨ì˜ ë¸Œëœë“œ ê°€ì¹˜ í‰ê°€ëŒ€íšŒë‹¤.\n",
      "ì§€ë‚œí•´ì— ì´ì–´ ì˜¬í•´ë„ ì´ 11ê°œ ë¶€ë¬¸ ìˆ˜ìƒì‘ì´ ì„ ì •ëë‹¤.\n",
      "ìˆ˜ìƒì‘ì€ â–³â€˜K-ë·°\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>')\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "text = 'ì ì„ ì˜ ìê¸° ìœ„í•´ í•  ìˆ˜ ìˆëŠ”ì¼ ì•Œë ¤ì¤˜'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì‚¬ìƒì„±\n",
    "# ì»¨í…ì¸  ìš”ì•½\n",
    "# ì§ˆì˜ ì‘ë‹µ\n",
    "# ëŒ€í™”í˜• AI\n",
    "# í…ìŠ¤íŠ¸ ì™„ì„±\n",
    "# ê°ì • ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shot\n",
    "def generated(prompt, examples):\n",
    "    prompt = f'{examples} \\n {prompt}'\n",
    "    model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    gen_ids = model.generate(input_ids,\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "    return tokenizer.decode(gen_ids[0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì˜ˆì‹œ1: ë¶í•œì˜ íŒŒë³‘ìœ¼ë¡œ êµ­ì œ ì •ì„¸ê°€ í¬ê²Œ ë™ìš”í•˜ê³  ìˆë‹¤. í‘¸í‹´ì€ íƒ€êµ­ì´ ê´€ì—¬í•˜ë©´ 3ì°¨ ì„¸ê³„ëŒ€ì „ì´ ì¼ì–´ë‚  ìˆ˜ ìˆë‹¤ê³  ê²½ê³ í–ˆë‹¤.\n",
      "ì˜ˆì‹œ2: í•œêµ­ ì •ë¶€ëŠ” ì´ ì‚¬íƒœë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ê³  ìˆê³  ë¬´ê¸°ë¥¼ ì œê³µí•  ê²ƒì„ ê³ ë ¤í•˜ê³  ìˆë‹¤\n",
      " \n",
      " ë¶í•œêµ°ì´ ì „íˆ¬ì— íˆ¬ì…ë˜ì—ˆì„ ë•Œ í•œêµ­ì˜ ë°˜ì‘ì€? #20180712 ì˜¤ëŠ˜ì˜ ì•„ì¹¨\n",
      "ì˜¤ëŠ˜ì˜ ì ì‹¬ : ë‹­ê°€ìŠ´ì‚´ìƒëŸ¬ë“œ (â‚©3,000)\n",
      "ë‹­ ê°€ìŠ´ì‚´ì€ ë‹¨ë°±ì§ˆê³¼ ë¹„íƒ€ë¯¼ì´ í’ë¶€í•´ì„œ ë‹¤ì´ì–´íŠ¸ì—ë„ ì¢‹ê³  ë§›ë„ ì¢‹ì•„ì„œ ìì£¼ ë¨¹ëŠ” í¸ì´ë¼ì„œ ë§ì´ ë¨¹ì—ˆì–´ìš”\n",
      "ë‹¤ì´ì–´íŠ¸ì— ì¢‹ì€ ìŒì‹ì¸ ë§Œí¼ ì˜¤ëŠ˜ í•˜ë£¨ë§Œì´ë¼ë„ ê¼­ ì±™ê²¨ë¨¹ì–´ì•¼ê² ì–´ìš”.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = '''\n",
    "ì˜ˆì‹œ1: ë¶í•œì˜ íŒŒë³‘ìœ¼ë¡œ êµ­ì œ ì •ì„¸ê°€ í¬ê²Œ ë™ìš”í•˜ê³  ìˆë‹¤. í‘¸í‹´ì€ íƒ€êµ­ì´ ê´€ì—¬í•˜ë©´ 3ì°¨ ì„¸ê³„ëŒ€ì „ì´ ì¼ì–´ë‚  ìˆ˜ ìˆë‹¤ê³  ê²½ê³ í–ˆë‹¤.\n",
    "ì˜ˆì‹œ2: í•œêµ­ ì •ë¶€ëŠ” ì´ ì‚¬íƒœë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ê³  ìˆê³  ë¬´ê¸°ë¥¼ ì œê³µí•  ê²ƒì„ ê³ ë ¤í•˜ê³  ìˆë‹¤\n",
    "'''\n",
    "news_prompt = 'ë¶í•œêµ°ì´ ì „íˆ¬ì— íˆ¬ì…ë˜ì—ˆì„ ë•Œ í•œêµ­ì˜ ë°˜ì‘ì€?'\n",
    "print(generated(news_prompt,example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 12:49:25.458500: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-05 12:49:25.458554: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 14506 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'staticmethod' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_directml\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4/lib/python3.9/site-packages/torch_directml/__init__.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# # Register backend to support AMP\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPrivateUse1Module\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_available\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a bool indicating if DML is currently available.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4/lib/python3.9/site-packages/torch_directml/__init__.py:74\u001b[0m, in \u001b[0;36mPrivateUse1Module\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisable_tiled_resources\u001b[39m(is_disabled):\n\u001b[1;32m     71\u001b[0m     torch_directml_native\u001b[38;5;241m.\u001b[39mdisable_tiled_resources(is_disabled)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_float64_support\u001b[39m(device_id \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_directml_native\u001b[38;5;241m.\u001b[39mhas_float64_support(device_id)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgpu_memory\u001b[39m(device_id \u001b[38;5;241m=\u001b[39m default_device(), mb_per_tile \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'staticmethod' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch_directml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
