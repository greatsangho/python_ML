{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "# os.environ['OPENAI_API_KEY'] = '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'\n",
    "os.environ.get('OpenAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# openai.api_key = ''\n",
    "response = openai.chat.completions.create(\n",
    "    # model= \"gpt-3.5-turbo\",\n",
    "    model= 'gpt-4o-mini',\n",
    "    messages = [\n",
    "        # {'role':'system', 'content':'You are a helpful assistant'},\n",
    "        # {'role':'user', 'content':'Who won the world series in 2020'},\n",
    "        # {'role':'assistant','content':'The Los Angeles Dodgers won the World Series in 2020'},\n",
    "        # {'role':'user', 'content':'Where was it played?'},\n",
    "        {'role':'user', 'content':'점심 메뉴를 추천해줘'}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'점심 메뉴로 몇 가지 추천드릴게요!\\n\\n1. **비빔밥**: 여러 가지 채소와 고기를 섞어서 고추장을 곁들여 먹는 건강한 한 끼입니다.\\n2. **김치찌개**: 따뜻하고 얼큰한 김치찌개에 밥과 함께 즐기면 좋습니다.\\n3. **샌드위치**: 간편하게 먹을 수 있는 샌드위치에 신선한 채소와 고기 또는 치즈를 넣어보세요.\\n4. **닭갈비**: 매콤한 양념으로 볶은 닭갈비는 밥과 함께 먹기 좋은 메뉴입니다.\\n5. **파스타**: 크림, 토마토, 오일 등 다양한 소스와 함께 즐길 수 있는 파스타도 좋은 선택입니다.\\n\\n어떤 메뉴가 마음에 드시나요?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5001\n",
      " * Running on http://172.21.70.32:5001\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# flask를 이용해서 \n",
    "import openai\n",
    "from flask import Flask, jsonify, request\n",
    "import os\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/prompt', methods = ['POST'])\n",
    "def generate_answer():    \n",
    "    if request.method == 'POST':\n",
    "        prompt = request.json['prompt']\n",
    "        openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "        pre_prompt = '한국어로 대답해줘\\n\\n'\n",
    "        response = openai.chat.completions.create(\n",
    "            model = \"gpt-3.5-turbo\",  # 무료\n",
    "            # model = \"gpt-4o\",  # 유로\n",
    "            messages = [\n",
    "                {'role':'system','content':'You ar a helpful assistant'},\n",
    "                {'role':'user','content':pre_prompt + prompt},                \n",
    "            ],\n",
    "            max_tokens = 3000,\n",
    "            stop = None,\n",
    "            temperature = 0.5  # 출력의 다양성  0.2 매우 보수적이고 일관성을유지: 간결한 답변,  0.8: 매우 창의적인 답변\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return jsonify({'answer' : answer})\n",
    "\n",
    "app.run(host=\"0.0.0.0\", debug=False, port=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdall-e-3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1024x1024\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m image_url \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39murl\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_url)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4/lib/python3.9/site-packages/openai/resources/images.py:264\u001b[0m, in \u001b[0;36mImages.generate\u001b[0;34m(self, prompt, model, n, quality, response_format, size, style, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    222\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ImagesResponse:\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    Creates an image given a prompt.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/images/generations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_generate_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageGenerateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mImagesResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4/lib/python3.9/site-packages/openai/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m     )\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4/lib/python3.9/site-packages/openai/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4/lib/python3.9/site-packages/openai/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "# image api\n",
    "# text prompt image DALL.E 3 / DALL.E 2\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "prompt = input(\"Prompt: \")\n",
    "response = client.images.generate(\n",
    "    model= \"dall-e-3\",\n",
    "    prompt=prompt,\n",
    "    size='1024x1024',\n",
    "    quality='hd',\n",
    "    n=1\n",
    ")\n",
    "image_url = response.data[0].url\n",
    "print(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 엔지니어링\n",
    "# 'Let's think step by step' 정답률이 2배 이상 상승\n",
    "# Multi-turn\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'근육을 만들기 위한 점심 메뉴로는 단백질과 영양소가 풍부한 음식을 추천합니다. 다음은 몇 가지 예시입니다:\\n\\n1. **닭 가슴살 샐러드**  \\n   - 재료: 닭 가슴살, 다양한 채소 (상추, 시금치, 방울토마토, 오이 등), 아보카도, 견과류, 올리브 오일, 레몬즙\\n   - 방법: 닭 가슴살을 굽고 채소와 함께 섞어 샐러드를 만든 후, 올리브 오일과 레몬즙으로 드레싱합니다.\\n\\n2. **고등어 구이와 퀴노아**  \\n   - 재료: 고등어, 퀴노아, 브로콜리, 올리브 오일, 소금, 후추\\n   - 방법: 고등어를 구워서 퀴노아와 찐 브로콜리와 함께 제공합니다.\\n\\n3. **터키 고기 보울**  \\n   - 재료: 다진 터키 고기, 현미 밥, 피망, 양파, 감자, 아보카도\\n   - 방법: 터키 고기를 볶아 밥과 함께 담고, 채소와 아보카도를 올려줍니다.\\n\\n4. **두부 스테이크와 채소 볶음**  \\n   - 재료: 두부, 시금치, 당근, 버섯, 간장, 올리브 오일\\n   - 방법: 두부를 두툼하게 썰어 구운 후, 채소를 볶아서 곁들입니다.\\n\\n이러한 메뉴는 단백질을 충분히 섭취하면서 영양 균형도 잘 맞출 수 있는 점심입니다. 적절한 운동과 함께 섭취하면 근육 빌딩에 도움이 됩니다!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "# openai.api_key = ''\n",
    "response = openai.chat.completions.create(\n",
    "    # model= \"gpt-3.5-turbo\",\n",
    "    model= 'gpt-4o-mini',\n",
    "    messages = [\n",
    "        {'role':'system', 'content':'안녕하세요 저는 당신의 영양사입니다. 다이어트 드오가 같이 식습관을 알려주시면 적절한 메뉴와 레시피를 알려드릴게요'},\n",
    "        {'role':'user', 'content':'다이어트 중인데 건강식으로 먹을 점심메뉴가 뭐가 있을까?'},\n",
    "        {'role':'assistant','content':'단호박 죽이 있으며 호박을 적당하게 쪄서 요리해야 합니다.'},\n",
    "        {'role':'user', 'content':'근육을 만들고 싶은데 점심은 뭘 먹을까?'}\n",
    "    ]\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝 - 미세조정\n",
    "# few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'감정: 긍정적'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# few shot\n",
    "import openai\n",
    "prompt = '''\n",
    "다음 리뷰의 감정을 판단해 주세요\n",
    "\n",
    "리뷰: \"이 영화는 정말 재미있어요! 강력 추천합니다.\"\n",
    "감정: 긍정적\n",
    "\n",
    "리뷰: \"시간 낭비한 기분입니다.\"\n",
    "감정: 부정적\n",
    "\n",
    "리뷰: \"정말 훌륭한 연기와 스토리\"\n",
    "감정:\n",
    "'''\n",
    "response = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini-2024-07-18',\n",
    "    messages=[{'role':'user', 'content':prompt}],\n",
    "    max_tokens= 10\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하나의 유기적인'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# openai로 학습한 모델을 다운로드 해서 사용하기\n",
    "import openai\n",
    "# ft:gpt-3.5-turbo-0125:personal::AQ3Qm0WA\n",
    "prompt = input(\"Prompt : \")\n",
    "response = openai.chat.completions.create(\n",
    "    model='ft:gpt-3.5-turbo-0125:personal::AQ3Qm0WA',\n",
    "    messages=[{'role':'user','content':prompt}],\n",
    "    max_tokens=10\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스기사 생성\n",
    "# 한국어 gpt2 모델 다운로드, 토크나이져\n",
    "# gpt2에 맞게 토큰화\n",
    "# 기사생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁안녕',\n",
       " '하',\n",
       " '세',\n",
       " '요.',\n",
       " '▁한국어',\n",
       " '▁G',\n",
       " 'P',\n",
       " 'T',\n",
       " '-2',\n",
       " '▁입',\n",
       " '니다.',\n",
       " '😤',\n",
       " ':)',\n",
       " 'l^o']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>')\n",
    "tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\")\n",
    "['▁안녕', '하', '세', '요.', '▁한국어', '▁G', 'P', 'T', '-2', '▁입', '니다.', '😤', ':)', 'l^o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잠을 잘 자기 위해 할 수 있는일 알려줘야 한다.\n",
      "또한 자신의 몸 상태를 체크해 볼 필요가 있다.\n",
      "이때는 평소보다 더 많은 양의 음식을 섭취해야 하며 특히 과일과 야채는 반드시 챙겨 먹어야 한다.</d> 지난달 30일 오후 서울 강남구 삼성동 코엑스 컨벤션홀. ‘2010 대한민국 대표브랜드 대상’ 시상식이 열렸다.\n",
      "올해로 3회째를 맞은 이 행사는 한국능률협회컨설팅(KMAC)이 주최하고 산업통상자원부, 지식경제부가 후원하는 국내 최대 규모의 브랜드 가치 평가대회다.\n",
      "지난해에 이어 올해도 총 11개 부문 수상작이 선정됐다.\n",
      "수상작은 △‘K-뷰\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>')\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "text = '잠을 잘 자기 위해 할 수 있는일 알려줘'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사생성\n",
    "# 컨텐츠 요약\n",
    "# 질의 응답\n",
    "# 대화형 AI\n",
    "# 텍스트 완성\n",
    "# 감정 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shot\n",
    "def generated(prompt, examples):\n",
    "    prompt = f'{examples} \\n {prompt}'\n",
    "    model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    gen_ids = model.generate(input_ids,\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "    return tokenizer.decode(gen_ids[0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "예시1: 북한의 파병으로 국제 정세가 크게 동요하고 있다. 푸틴은 타국이 관여하면 3차 세계대전이 일어날 수 있다고 경고했다.\n",
      "예시2: 한국 정부는 이 사태를 중요하게 생각하고 있고 무기를 제공할 것을 고려하고 있다\n",
      " \n",
      " 북한군이 전투에 투입되었을 때 한국의 반응은? #20180712 오늘의 아침\n",
      "오늘의 점심 : 닭가슴살샐러드 (₩3,000)\n",
      "닭 가슴살은 단백질과 비타민이 풍부해서 다이어트에도 좋고 맛도 좋아서 자주 먹는 편이라서 많이 먹었어요\n",
      "다이어트에 좋은 음식인 만큼 오늘 하루만이라도 꼭 챙겨먹어야겠어요.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = '''\n",
    "예시1: 북한의 파병으로 국제 정세가 크게 동요하고 있다. 푸틴은 타국이 관여하면 3차 세계대전이 일어날 수 있다고 경고했다.\n",
    "예시2: 한국 정부는 이 사태를 중요하게 생각하고 있고 무기를 제공할 것을 고려하고 있다\n",
    "'''\n",
    "news_prompt = '북한군이 전투에 투입되었을 때 한국의 반응은?'\n",
    "print(generated(news_prompt,example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 12:49:25.458500: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-05 12:49:25.458554: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 14506 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'staticmethod' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_directml\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4/lib/python3.9/site-packages/torch_directml/__init__.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# # Register backend to support AMP\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPrivateUse1Module\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_available\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a bool indicating if DML is currently available.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4/lib/python3.9/site-packages/torch_directml/__init__.py:74\u001b[0m, in \u001b[0;36mPrivateUse1Module\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisable_tiled_resources\u001b[39m(is_disabled):\n\u001b[1;32m     71\u001b[0m     torch_directml_native\u001b[38;5;241m.\u001b[39mdisable_tiled_resources(is_disabled)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_float64_support\u001b[39m(device_id \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_directml_native\u001b[38;5;241m.\u001b[39mhas_float64_support(device_id)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgpu_memory\u001b[39m(device_id \u001b[38;5;241m=\u001b[39m default_device(), mb_per_tile \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'staticmethod' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch_directml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
