{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtgiJvcEVS3l",
    "outputId": "d947291b-75da-4492-88de-8d5330c703b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xnli.test.ko.tsv', <http.client.HTTPMessage at 0x2402f723040>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 카카오에서 공개한 한국어 벤치마크 데이터셋\n",
    "# 문장이 두개.. 두개의 문장의 관계를 3개의 클래스로 구분\n",
    "# 수반(entailment) 중립(netual) 모순(contradiction)\n",
    "import urllib\n",
    "# 훈련 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/multinli.train.ko.tsv\", filename=\"multinli.train.ko.tsv\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/snli_1.0_train.ko.tsv\", filename=\"snli_1.0_train.ko.tsv\")\n",
    "\n",
    "# 검증 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/xnli.dev.ko.tsv\", filename=\"xnli.dev.ko.tsv\")\n",
    "\n",
    "# 테스트 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/xnli.test.ko.tsv\", filename=\"xnli.test.ko.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0EYOf-iVVZb_"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 클래스\n",
    "# 모델클래스 : GPT2 로드하고 마지막층에 Linear층 --> 분류기 추가.\n",
    "# 손실함수... 이진 BinaryCrossEntropy, 다중 CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wcfTWFG1Vc31"
   },
   "outputs": [],
   "source": [
    "# 데이터 확인\n",
    "import pandas as pd\n",
    "train_multi = pd.read_csv('multinli.train.ko.tsv',sep='\\t',quoting=3)\n",
    "train_snli = pd.read_csv('snli_1.0_train.ko.tsv',sep='\\t',quoting=3)\n",
    "val_data = pd.read_csv('xnli.dev.ko.tsv',sep='\\t',quoting=3)\n",
    "test_data = pd.read_csv('xnli.test.ko.tsv',sep='\\t',quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hj2n_paFVwHQ",
    "outputId": "bc375265-06c7-4c67-8e98-32ccfc19d886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 942854 entries, 0 to 942853\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   sentence1   942854 non-null  object\n",
      " 1   sentence2   942808 non-null  object\n",
      " 2   gold_label  942854 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 21.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.concat([train_multi,train_snli]).reset_index(drop=True)\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VfSBfz1SVeE9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "luLefDvTVfv0"
   },
   "outputs": [],
   "source": [
    "class KorNLIDataset(Dataset):\n",
    "  def __init__(self,sentence1, sentence2,labels,tokenizer,max_length) -> None:\n",
    "    self.sentence1 = sentence1\n",
    "    self.sentence2 = sentence2\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length = max_length\n",
    "  def __len__(self):\n",
    "    return len(self.sentence1)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # 각 문장에 대해서 토큰나이저\n",
    "    sent1 = self.sentence1[index]\n",
    "    sent2 = self.sentence2[index]\n",
    "    inputs =  self.tokenizer(\n",
    "        text = sent1,\n",
    "        text_pair = sent2,\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        max_length = self.max_length,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].squeeze(0)\n",
    "    attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "    label = torch.tensor(self.labels[index])\n",
    "    return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMQMgflOZJ47"
   },
   "source": [
    "# 데이터셋 테스트를 위한 셈플코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RolEQyLJVkHn"
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2',\n",
    "#                                           bos_token='</s>', eos_token='</s>', pad_token='<pad>')\n",
    "# sent1, sent2 = train_data['sentence1'].to_list(),train_data['sentence2'].to_list()\n",
    "# inputs =tokenizer(\n",
    "#         text = sent1[0],\n",
    "#         text_pair = sent2[0],\n",
    "#         truncation = True,\n",
    "#         padding = 'max_length',\n",
    "#         max_length = 127,\n",
    "#         return_tensors = 'pt'\n",
    "# )\n",
    "# print(f'key : {inputs.keys()}')\n",
    "# print(f\"input_ids shape : { inputs['input_ids'].squeeze(0).shape , inputs['attention_mask'].squeeze(0).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8ff-GqJZbpB"
   },
   "source": [
    "# 데이터셋 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ptZdowUlX-jl"
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2',\n",
    "#                                            bos_token='</s>', eos_token='</s>', pad_token='<pad>')\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# encoded_label = le.fit_transform(train_data['gold_label'])\n",
    "# print(le.classes_, le.transform(le.classes_))\n",
    "\n",
    "# train_dataset = KorNLIDataset(train_data['sentence1'].to_list()\n",
    "#   ,train_data['sentence2'].to_list()\n",
    "#   ,encoded_label\n",
    "#   ,tokenizer\n",
    "#   ,127)\n",
    "\n",
    "# input_ids, attention_mask, label = next(iter(train_dataset))\n",
    "# print(f'input_ids shape : {input_ids.shape}')\n",
    "# print(f'attention_mask shape : {attention_mask.shape}')\n",
    "# print(f'label : {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "z5y1rZbiZQMw",
    "outputId": "b65fffb4-3ec1-4a98-bd82-c7c74db925f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               neutral\n",
       "1            entailment\n",
       "2            entailment\n",
       "3            entailment\n",
       "4               neutral\n",
       "              ...      \n",
       "942849    contradiction\n",
       "942850          neutral\n",
       "942851          neutral\n",
       "942852    contradiction\n",
       "942853       entailment\n",
       "Name: gold_label, Length: 942854, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['gold_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAFUZsdPhjXM"
   },
   "source": [
    "# 모델정의(클래스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mrBIfKOdh2T_"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "class GPT2ForSeqClassification(torch.nn.Module):\n",
    "  def __init__(self, num_labels):\n",
    "    super(GPT2ForSeqClassification, self).__init__()\n",
    "    self.num_labels = num_labels\n",
    "    self.gpt = GPT2Model.from_pretrained('skt/kogpt2-base-v2')  # 128*6\n",
    "    # 분류기를 통과\n",
    "    self.classifier = torch.nn.Linear(768, self.num_labels)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    outputs =  self.gpt(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    cls_output =  outputs.last_hidden_state[:,-1,:]  # 입력 텍스트의 요약본\n",
    "    logits = self.classifier(cls_output)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zt7HJJBjhrD",
    "outputId": "4f419d1a-3118-4464-e678-c39142e9f6d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\tcml\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contradiction' 'entailment' 'neutral'] [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2',\n",
    "                                           bos_token='</s>', eos_token='</s>', pad_token='<pad>')\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_data['gold_label'])\n",
    "y_val= le.transform(val_data['gold_label'])\n",
    "y_test= le.transform(test_data['gold_label'])\n",
    "print(le.classes_, le.transform(le.classes_))\n",
    "\n",
    "train_dataset = KorNLIDataset(train_data['sentence1'].to_list()\n",
    "  ,train_data['sentence2'].to_list()\n",
    "  ,y_train\n",
    "  ,tokenizer\n",
    "  ,127\n",
    ")\n",
    "val_dataset = KorNLIDataset(val_data['sentence1'].to_list()\n",
    "  ,val_data['sentence2'].to_list()\n",
    "  ,y_val\n",
    "  ,tokenizer\n",
    "  ,127\n",
    ")\n",
    "test_dataset = KorNLIDataset(test_data['sentence1'].to_list()\n",
    "  ,test_data['sentence2'].to_list()\n",
    "  ,y_test\n",
    "  ,tokenizer\n",
    "  ,127\n",
    ")\n",
    "# 로더\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbnwxSWAkj74"
   },
   "source": [
    "# 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ovo02JlDkfhP",
    "outputId": "701c72f2-6be6-43b7-c6df-73306b84d5ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\tcml\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\tcml\\lib\\site-packages\\transformers\\modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSeqClassification(\n",
       "  (gpt): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_directml\n",
    "device = torch_directml.device() # torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = GPT2ForSeqClassification(3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fpwlkMCk4ZP"
   },
   "source": [
    "# 옵티마이저 및 손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "w5k4Xp9sk3ks"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVeXf__YlHua"
   },
   "source": [
    "# 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "c3uZrOs6lDo-",
    "outputId": "587739ed-2fa2-42a6-fe61-22bb81e706ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29465 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The GPU device does not support Double (Float64) operations!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 예측\u001b[39;00m\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, label)    \u001b[38;5;66;03m# 손실값\u001b[39;00m\n\u001b[0;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# 기울기 구함\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\tcml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\tcml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m, in \u001b[0;36mGPT2ForSeqClassification.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[1;32m---> 11\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m   cls_output \u001b[38;5;241m=\u001b[39m  outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]  \u001b[38;5;66;03m# 입력 텍스트의 요약본\u001b[39;00m\n\u001b[0;32m     13\u001b[0m   logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(cls_output)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\tcml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\tcml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\tcml\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:826\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# positions we want to attend and the dtype's smallest value for masked positions.\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;66;03m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[0;32m    825\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)  \u001b[38;5;66;03m# fp16 compatibility\u001b[39;00m\n\u001b[1;32m--> 826\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[0;32m    828\u001b[0m \u001b[38;5;66;03m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39madd_cross_attention \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\tcml\\lib\\site-packages\\torch\\_tensor.py:41\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\tcml\\lib\\site-packages\\torch\\_tensor.py:962\u001b[0m, in \u001b[0;36mTensor.__rsub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VariableFunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The GPU device does not support Double (Float64) operations!"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(2):\n",
    "  iterator = tqdm(train_loader)\n",
    "  for input_ids, attention_mask, label in iterator:\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask) # 예측\n",
    "    loss = loss_fn(outputs, label)    # 손실값\n",
    "    loss.backward() # 기울기 구함\n",
    "    optimizer.step() # 기울기 업데이트\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    iterator.set_description(f'Epoch {epoch}')\n",
    "    iterator.set_postfix_str(f'Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to load PyTorch C extensions:\n    It appears that PyTorch has loaded the `torch/_C` folder\n    of the PyTorch repository rather than the C extensions which\n    are expected in the `torch._C` namespace. This can occur when\n    using the `install` workflow. e.g.\n        $ python setup.py install && python -c \"import torch\"\n\n    This error can generally be solved using the `develop` workflow\n        $ python setup.py develop && python -c \"import torch\"  # This should succeed\n    or by running Python from a different directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_directml\u001b[39;00m\n\u001b[0;32m      3\u001b[0m dml \u001b[38;5;241m=\u001b[39m torch_directml\u001b[38;5;241m.\u001b[39mdevice()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pytdml\\lib\\site-packages\\torch\\__init__.py:749\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;66;03m# The __file__ check only works for Python 3.7 and above.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _C_for_compiled_check\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 749\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124m                of the PyTorch repository rather than the C extensions which\u001b[39m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124m                are expected in the `torch._C` namespace. This can occur when\u001b[39m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;124m                using the `install` workflow. e.g.\u001b[39m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;124m                    $ python setup.py install && python -c \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport torch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    756\u001b[0m \n\u001b[0;32m    757\u001b[0m \u001b[38;5;124m                This error can generally be solved using the `develop` workflow\u001b[39m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;124m                    $ python setup.py develop && python -c \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport torch\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  # This should succeed\u001b[39m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[0;32m    763\u001b[0m __name, __obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to load PyTorch C extensions:\n    It appears that PyTorch has loaded the `torch/_C` folder\n    of the PyTorch repository rather than the C extensions which\n    are expected in the `torch._C` namespace. This can occur when\n    using the `install` workflow. e.g.\n        $ python setup.py install && python -c \"import torch\"\n\n    This error can generally be solved using the `develop` workflow\n        $ python setup.py develop && python -c \"import torch\"  # This should succeed\n    or by running Python from a different directory."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "dml = torch_directml.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VxCGxKowmO0Y"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39m__version__\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.tensor([1]).to(dml) # Note that dml is a variable, not a string!\n",
    "tensor2 = torch.tensor([2]).to(dml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_algebra = tensor1 + tensor2\n",
    "dml_algebra.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor1 = torch.tensor([1]).to(dml)\n",
    "tensor2 = torch.tensor([2]).to(dml)\n",
    "dml_algebra = tensor1 + tensor2\n",
    "dml_algebra.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
