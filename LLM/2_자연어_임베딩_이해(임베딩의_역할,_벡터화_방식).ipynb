{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "etr-QC9uRQdm",
        "outputId": "ff8bcda4-d9c9-4e45-aa4e-5f3cfc588259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.11.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.0)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.0)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.64.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.23.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "525579e666484b0d8906edc452d1058a",
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.17.0'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install tensorflow==2.8.0\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Th_N7nXnSo1x",
        "outputId": "b57720ff-615e-4561-e78c-e8b4035d0e07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-23 17:28:54.525092: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-23 17:28:54.741954: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-23 17:28:56.304891: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdirectml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.so\n",
            "2024-10-23 17:28:56.304959: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdxcore.so\n",
            "2024-10-23 17:28:56.313173: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libd3d12.so\n",
            "2024-10-23 17:28:56.954581: I tensorflow/c/logging.cc:34] DirectML device enumeration: found 1 compatible adapters.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'2.10.0'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "QJhud4CxHRsO",
        "outputId": "6d7f3753-da29-454c-bc70-d69f6cc2fab0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rZyJfHvr66Gcn0yRWxts-1w6zKLEYDhp\n",
            "To: /mnt/c/Github/python_ML/LLM/data_short.csv\n",
            "100%|██████████| 1.45k/1.45k [00:00<00:00, 3.13MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'data_short.csv'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "file_id = '1rZyJfHvr66Gcn0yRWxts-1w6zKLEYDhp'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "gdown.download(download_url, 'data_short.csv', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WqXeE_DgINzU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzSfXkLqHcWV",
        "outputId": "7c5bb8e7-03c6-4a1e-fb1f-c99980d49be5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20, 3)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_short = pd.read_csv('data_short.csv')\n",
        "data_short.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R28vcrWjIWIu",
        "outputId": "d87db258-719d-4556-ca60-6eb3e7dcc43b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /home/sangho/miniconda3/envs/tfdml_plugin/lib/python3.7/site-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /home/sangho/miniconda3/envs/tfdml_plugin/lib/python3.7/site-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /home/sangho/miniconda3/envs/tfdml_plugin/lib/python3.7/site-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /home/sangho/miniconda3/envs/tfdml_plugin/lib/python3.7/site-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: packaging in /home/sangho/miniconda3/envs/tfdml_plugin/lib/python3.7/site-packages (from JPype1>=0.7.0->konlpy) (22.0)\n",
            "Requirement already satisfied: typing-extensions in /home/sangho/miniconda3/envs/tfdml_plugin/lib/python3.7/site-packages (from JPype1>=0.7.0->konlpy) (4.4.0)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rdvJ_G3gH91E"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "FILTER = string.punctuation\n",
        "PAD = \"<PAD>\"  # 패딩\n",
        "STD = \"<SOS>\"\n",
        "END = '<END>'\n",
        "UNK = '<UNK>'  # 사전에 없는단어\n",
        "PAD_INDEX = 0\n",
        "STD_INDEX = 1\n",
        "END_INDEX = 2\n",
        "UNK_INDEX = 3\n",
        "MARKER = [PAD,STD,END,UNK]\n",
        "MAX_SEQUENCE = 25  # 이거보다 크면 자르고 짧으면 패딩으로 채운다\n",
        "# load_data(path)  csv 를 읽어서 질문과 답변 question, answer\n",
        "# data_tokenizer(data) 특수문자를 제거 단어를 토큰으로 나눠서 리스트에 저장\n",
        "# prepro_like_morphlized(data) 형태소 분석기를사용해서 문장을 형태소 단위로 나눈다\n",
        "# load_vocabulary(path,vocab_size,tokenize_as_morph=False) 사전을 생성하거나 불러온다 학습에 사용할 단어사전만들거나 불러올때\n",
        "# make_vocabulary(vocabulary_list) 단어 리스트 seq2idx(단어->인덱스), idx2seq(인덱스->단어) 딕셔너리\n",
        "# enc_processing(value,dictionary, tokenize_as_morph=False) 문장을 인덱스로 변환\n",
        "# dec_output_processing(value, dictionary, tokenize_as_morph=False) 디코더의 입력데이터를 인덱스\n",
        "# dec_target_processing(value, dictionary, tokenize_as_morph=False) 디코더의 타겟(출력) 인덱\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "PAD = \"<PAD>\"\n",
        "STD = \"<SOS>\"\n",
        "END = \"<END>\"\n",
        "UNK = \"<UNK>\"\n",
        "\n",
        "PAD_INDEX = 0\n",
        "STD_INDEX = 1\n",
        "END_INDEX = 2\n",
        "UNK_INDEX = 3\n",
        "\n",
        "MARKER = [PAD, STD, END, UNK]\n",
        "CHANGE_FILTER = re.compile(FILTERS)\n",
        "\n",
        "MAX_SEQUENCE = 25\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "    # 판다스를 통해서 데이터를 불러온다.\n",
        "    data_df = pd.read_csv(path, header=0)\n",
        "    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
        "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "\n",
        "    return question, answer\n",
        "\n",
        "\n",
        "def data_tokenizer(data):\n",
        "    # 토크나이징 해서 담을 배열 생성\n",
        "    words = []\n",
        "    for sentence in data:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 위 필터와 같은 값들을 정규화 표현식을\n",
        "        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
        "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
        "        for word in sentence.split():\n",
        "            words.append(word)\n",
        "    # 토그나이징과 정규표현식을 통해 만들어진\n",
        "    # 값들을 넘겨 준다.\n",
        "    return [word for word in words if word]\n",
        "\n",
        "\n",
        "def prepro_like_morphlized(data):\n",
        "    morph_analyzer = Okt()\n",
        "    result_data = list()\n",
        "    for seq in tqdm(data):\n",
        "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
        "        result_data.append(morphlized_seq)\n",
        "\n",
        "    return result_data\n",
        "\n",
        "\n",
        "def load_vocabulary(path, vocab_path, tokenize_as_morph=False):\n",
        "    # 사전을 담을 배열 준비한다.\n",
        "    vocabulary_list = []\n",
        "    # 사전을 구성한 후 파일로 저장 진행한다.\n",
        "    # 그 파일의 존재 유무를 확인한다.\n",
        "    if not os.path.exists(vocab_path):\n",
        "        # 이미 생성된 사전 파일이 존재하지 않으므로\n",
        "        # 데이터를 가지고 만들어야 한다.\n",
        "        # 그래서 데이터가 존재 하면 사전을 만들기 위해서\n",
        "        # 데이터 파일의 존재 유무를 확인한다.\n",
        "        if (os.path.exists(path)):\n",
        "            # 데이터가 존재하니 판단스를 통해서\n",
        "            # 데이터를 불러오자\n",
        "            data_df = pd.read_csv(path, encoding='utf-8')\n",
        "            # 판다스의 데이터 프레임을 통해서\n",
        "            # 질문과 답에 대한 열을 가져 온다.\n",
        "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "            if tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n",
        "                question = prepro_like_morphlized(question)\n",
        "                answer = prepro_like_morphlized(answer)\n",
        "            data = []\n",
        "            # 질문과 답변을 extend을\n",
        "            # 통해서 구조가 없는 배열로 만든다.\n",
        "            data.extend(question)\n",
        "            data.extend(answer)\n",
        "            # 토큰나이져 처리 하는 부분이다.\n",
        "            words = data_tokenizer(data)\n",
        "            # 공통적인 단어에 대해서는 모두\n",
        "            # 필요 없으므로 한개로 만들어 주기 위해서\n",
        "            # set해주고 이것들을 리스트로 만들어 준다.\n",
        "            words = list(set(words))\n",
        "            # 데이터 없는 내용중에 MARKER를 사전에\n",
        "            # 추가 하기 위해서 아래와 같이 처리 한다.\n",
        "            # 아래는 MARKER 값이며 리스트의 첫번째 부터\n",
        "            # 순서대로 넣기 위해서 인덱스 0에 추가한다.\n",
        "            # PAD = \"<PADDING>\"\n",
        "            # STD = \"<START>\"\n",
        "            # END = \"<END>\"\n",
        "            # UNK = \"<UNKNWON>\"\n",
        "            words[:0] = MARKER\n",
        "        # 사전을 리스트로 만들었으니 이 내용을\n",
        "        # 사전 파일을 만들어 넣는다.\n",
        "        with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n",
        "            for word in words:\n",
        "                vocabulary_file.write(word + '\\n')\n",
        "\n",
        "    # 사전 파일이 존재하면 여기에서\n",
        "    # 그 파일을 불러서 배열에 넣어 준다.\n",
        "    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n",
        "        for line in vocabulary_file:\n",
        "            vocabulary_list.append(line.strip())\n",
        "\n",
        "    # 배열에 내용을 키와 값이 있는\n",
        "    # 딕셔너리 구조로 만든다.\n",
        "    char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
        "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다.\n",
        "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
        "    return char2idx, idx2char, len(char2idx)\n",
        "\n",
        "\n",
        "def make_vocabulary(vocabulary_list):\n",
        "    # 리스트를 키가 단어이고 값이 인덱스인\n",
        "    # 딕셔너리를 만든다.\n",
        "    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
        "    # 리스트를 키가 인덱스이고 값이 단어인\n",
        "    # 딕셔너리를 만든다.\n",
        "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
        "    # 두개의 딕셔너리를 넘겨 준다.\n",
        "    return char2idx, idx2char\n",
        "\n",
        "\n",
        "def enc_processing(value, dictionary, tokenize_as_morph=False):\n",
        "    # 인덱스 값들을 가지고 있는\n",
        "    # 배열이다.(누적된다.)\n",
        "    sequences_input_index = []\n",
        "    # 하나의 인코딩 되는 문장의\n",
        "    # 길이를 가지고 있다.(누적된다.)\n",
        "    sequences_length = []\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if tokenize_as_morph:\n",
        "        value = prepro_like_morphlized(value)\n",
        "\n",
        "    # 한줄씩 불어온다.\n",
        "    for sequence in value:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 정규화를 사용하여 필터에 들어 있는\n",
        "        # 값들을 \"\" 으로 치환 한다.\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        # 하나의 문장을 인코딩 할때\n",
        "        # 가지고 있기 위한 배열이다.\n",
        "        sequence_index = []\n",
        "        # 문장을 스페이스 단위로\n",
        "        # 자르고 있다.\n",
        "        for word in sequence.split():\n",
        "            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고\n",
        "            # 그 값을 가져와 sequence_index에 추가한다.\n",
        "            if dictionary.get(word) is not None:\n",
        "                sequence_index.extend([dictionary[word]])\n",
        "            # 잘려진 단어가 딕셔너리에 존재 하지 않는\n",
        "            # 경우 이므로 UNK(2)를 넣어 준다.\n",
        "            else:\n",
        "                sequence_index.extend([dictionary[UNK]])\n",
        "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "        if len(sequence_index) > MAX_SEQUENCE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "        # 하나의 문장에 길이를 넣어주고 있다.\n",
        "        sequences_length.append(len(sequence_index))\n",
        "        # max_sequence_length보다 문장 길이가\n",
        "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "        # 인덱스화 되어 있는 값을\n",
        "        # sequences_input_index에 넣어 준다.\n",
        "        sequences_input_index.append(sequence_index)\n",
        "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
        "    # 사전 작업이다.\n",
        "    # 넘파이 배열에 인덱스화된 배열과\n",
        "    # 그 길이를 넘겨준다.\n",
        "    return np.asarray(sequences_input_index), sequences_length\n",
        "\n",
        "\n",
        "def dec_output_processing(value, dictionary, tokenize_as_morph=False):\n",
        "    # 인덱스 값들을 가지고 있는\n",
        "    # 배열이다.(누적된다)\n",
        "    sequences_output_index = []\n",
        "    # 하나의 디코딩 입력 되는 문장의\n",
        "    # 길이를 가지고 있다.(누적된다)\n",
        "    sequences_length = []\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if tokenize_as_morph:\n",
        "        value = prepro_like_morphlized(value)\n",
        "    # 한줄씩 불어온다.\n",
        "    for sequence in value:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 정규화를 사용하여 필터에 들어 있는\n",
        "        # 값들을 \"\" 으로 치환 한다.\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        # 하나의 문장을 디코딩 할때 가지고\n",
        "        # 있기 위한 배열이다.\n",
        "        sequence_index = []\n",
        "        # 디코딩 입력의 처음에는 START가 와야 하므로\n",
        "        # 그 값을 넣어 주고 시작한다.\n",
        "        # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의\n",
        "        # 값인 인덱스를 넣어 준다.\n",
        "        sequence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "        if len(sequence_index) > MAX_SEQUENCE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "        # 하나의 문장에 길이를 넣어주고 있다.\n",
        "        sequences_length.append(len(sequence_index))\n",
        "        # max_sequence_length보다 문장 길이가\n",
        "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "        # 인덱스화 되어 있는 값을\n",
        "        # sequences_output_index 넣어 준다.\n",
        "        sequences_output_index.append(sequence_index)\n",
        "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
        "    # 사전 작업이다.\n",
        "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
        "    return np.asarray(sequences_output_index), sequences_length\n",
        "\n",
        "\n",
        "def dec_target_processing(value, dictionary, tokenize_as_morph=False):\n",
        "    # 인덱스 값들을 가지고 있는\n",
        "    # 배열이다.(누적된다)\n",
        "    sequences_target_index = []\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if tokenize_as_morph:\n",
        "        value = prepro_like_morphlized(value)\n",
        "    # 한줄씩 불어온다.\n",
        "    for sequence in value:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 정규화를 사용하여 필터에 들어 있는\n",
        "        # 값들을 \"\" 으로 치환 한다.\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        # 문장에서 스페이스 단위별로 단어를 가져와서\n",
        "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
        "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
        "        sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "        # 그리고 END 토큰을 넣어 준다\n",
        "        if len(sequence_index) >= MAX_SEQUENCE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]\n",
        "        else:\n",
        "            sequence_index += [dictionary[END]]\n",
        "        # max_sequence_length보다 문장 길이가\n",
        "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "        # 인덱스화 되어 있는 값을\n",
        "        # sequences_target_index에 넣어 준다.\n",
        "        sequences_target_index.append(sequence_index)\n",
        "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
        "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
        "    return np.asarray(sequences_target_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Qdz7vPZ_H_sN"
      },
      "outputs": [],
      "source": [
        "inputs,outputs = load_data('data_short.csv')\n",
        "char2idx,idx2char,vocab_size =  load_vocabulary('data_short.csv','vocabulary.txt')\n",
        "index_inputs,input_seq_len = enc_processing(inputs,char2idx)\n",
        "index_outputs,output_seq_len = dec_output_processing(outputs,char2idx)\n",
        "index_targets = dec_target_processing(outputs,char2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "g_bYy6w4ICzs"
      },
      "outputs": [],
      "source": [
        "data_configs = {}\n",
        "data_configs['char2idx'] = char2idx\n",
        "data_configs['idx2char'] = idx2char\n",
        "data_configs['vocab_size'] = vocab_size\n",
        "data_configs['pad_symbol'] = PAD\n",
        "data_configs['std_symbol'] = STD\n",
        "data_configs['end_symbol'] = END\n",
        "data_configs['unk_symbol'] = UNK\n",
        "\n",
        "TRAIN_INPUTS = 'train_inputs.npy'\n",
        "TRAIN_OUTPUTS ='train_outputs.npy'\n",
        "TRAIN_TARGETS ='train_targets.npy'\n",
        "DATA_CONFIGS ='data_configs.json'\n",
        "\n",
        "np.save(open(TRAIN_INPUTS, 'wb'), index_inputs)\n",
        "np.save(open(TRAIN_OUTPUTS, 'wb'), index_outputs)\n",
        "np.save(open(TRAIN_TARGETS, 'wb'), index_targets)\n",
        "\n",
        "json.dump(data_configs, open(DATA_CONFIGS, 'w'))\n",
        "\n",
        "json.dump(data_configs,open('data_configs.json','w'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihrF1QhoIbUk"
      },
      "source": [
        "# 모델 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FzxTN3OOIdW9"
      },
      "outputs": [],
      "source": [
        "index_inputs = np.load(open(TRAIN_INPUTS, 'rb'))\n",
        "index_outputs = np.load(open(TRAIN_OUTPUTS , 'rb'))\n",
        "index_targets = np.load(open(TRAIN_TARGETS , 'rb'))\n",
        "prepro_configs = json.load(open(DATA_CONFIGS, 'r'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlSrpfBOInJz",
        "outputId": "9781b125-a445-462b-a654-3ffa2e8186e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 20 20\n"
          ]
        }
      ],
      "source": [
        "print(len(index_inputs),  len(index_outputs), len(index_targets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ndMQKg3cIk2U"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'seq2seq_kor'\n",
        "BATCH_SIZE = 2\n",
        "MAX_SEQUENCE = 25\n",
        "EPOCH = 30\n",
        "UNITS = 1024\n",
        "EMBEDDING_DIM = 256\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "char2idx = prepro_configs['char2idx']\n",
        "idx2char = prepro_configs['idx2char']\n",
        "std_index = prepro_configs['std_symbol']\n",
        "end_index = prepro_configs['end_symbol']\n",
        "vocab_size = prepro_configs['vocab_size']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Yyl1_ytEIryb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0zCq2nSqIpBa"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "      super(Encoder, self).__init__()\n",
        "      self.batch_sz = batch_sz\n",
        "      self.enc_units = enc_units\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embedding_dim = embedding_dim\n",
        "\n",
        "      self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
        "      self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                      return_sequences=True,\n",
        "                                      return_state=True,\n",
        "                                      recurrent_initializer='glorot_uniform'\n",
        "                                      )\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "      x = self.embedding(x)\n",
        "      output, state = self.gru(x, initial_state = hidden)\n",
        "      return output, state\n",
        "\n",
        "    def initialize_hidden_state(self, inp):\n",
        "      return tf.zeros((tf.shape(inp)[0], self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "i1A1lxJVIqty"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zZWok_YUIwOq"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        output, state = self.gru(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jqZcqNrtIyMa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-23 17:46:01.692655: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-23 17:46:01.697054: I tensorflow/c/logging.cc:34] DirectML: creating device on adapter 0 (Intel(R) Iris(R) Xe Graphics)\n",
            "2024-10-23 17:46:01.800222: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-10-23 17:46:01.800267: W tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.cc:28] Overriding allow_growth setting because force_memory_growth was requested by the device.\n",
            "2024-10-23 17:46:01.800289: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14506 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "def loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def accuracy(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
        "    pred *= mask\n",
        "    acc = train_accuracy(real, pred)\n",
        "\n",
        "    return tf.reduce_mean(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uHnBTFTGIzoi"
      },
      "outputs": [],
      "source": [
        "class seq2seq(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, dec_units, batch_sz, end_token_idx=2):\n",
        "        super(seq2seq, self).__init__()\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, batch_sz)\n",
        "        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_sz)\n",
        "\n",
        "    def call(self, x):\n",
        "        inp, tar = x\n",
        "\n",
        "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
        "        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        predict_tokens = list()\n",
        "        for t in range(0, tar.shape[1]):\n",
        "            dec_input = tf.dtypes.cast(tf.expand_dims(tar[:, t], 1), tf.float32)\n",
        "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
        "            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))\n",
        "        return tf.stack(predict_tokens, axis=1)\n",
        "\n",
        "    def inference(self, x):\n",
        "        inp  = x\n",
        "\n",
        "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
        "        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([char2idx[std_index]], 1)\n",
        "\n",
        "        predict_tokens = list()\n",
        "        for t in range(0, MAX_SEQUENCE):\n",
        "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
        "            predict_token = tf.argmax(predictions[0])\n",
        "\n",
        "            if predict_token == self.end_token_idx:\n",
        "                break\n",
        "\n",
        "            predict_tokens.append(predict_token)\n",
        "            dec_input = tf.dtypes.cast(tf.expand_dims([predict_token], 0), tf.float32)\n",
        "\n",
        "        return tf.stack(predict_tokens, axis=0).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0JbUUBTsI1qr"
      },
      "outputs": [],
      "source": [
        "model = seq2seq(vocab_size, EMBEDDING_DIM, UNITS, UNITS, BATCH_SIZE, char2idx[end_index])\n",
        "model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(1e-3), metrics=[accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6yhzV3bI3ES",
        "outputId": "36890ee7-06e1-4273-bab7-71dc37174599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-23 17:46:17.909222: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-10-23 17:46:20.941212: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-10-23 17:46:20.941273: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14506 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
            "2024-10-23 17:46:20.943593: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-10-23 17:46:20.943643: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14506 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
            "2024-10-23 17:46:20.951563: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-10-23 17:46:20.951645: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14506 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
            "2024-10-23 17:46:20.953464: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-10-23 17:46:20.953572: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14506 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
            "2024-10-23 17:46:21.280071: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at partitioned_function_ops.cc:115 : INVALID_ARGUMENT: No OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [dropout=0, seed=0, input_mode=\"linear_input\", T=DT_FLOAT, direction=\"unidirectional\", rnn_mode=\"gru\", is_training=true, seed2=0]\n",
            "Registered devices: [CPU, GPU]\n",
            "Registered kernels:\n",
            "  <no registered kernels>\n",
            "\n",
            "\t [[CudnnRNN]]\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nNo OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [dropout=0, seed=0, input_mode=\"linear_input\", T=DT_FLOAT, direction=\"unidirectional\", rnn_mode=\"gru\", is_training=true, seed2=0]\nRegistered devices: [CPU, GPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[CudnnRNN]]\n\t [[seq2seq/encoder/gru/PartitionedCall]] [Op:__inference_train_function_45963]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_32413/4266235191.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m history = model.fit([index_inputs, index_outputs], index_targets,\n\u001b[1;32m     13\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                     validation_split=VALIDATION_SPLIT, callbacks=[earlystop_callback, cp_callback])\n\u001b[0m",
            "\u001b[0;32m~/miniconda3/envs/tfdml_plugin/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/tfdml_plugin/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nNo OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [dropout=0, seed=0, input_mode=\"linear_input\", T=DT_FLOAT, direction=\"unidirectional\", rnn_mode=\"gru\", is_training=true, seed2=0]\nRegistered devices: [CPU, GPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[CudnnRNN]]\n\t [[seq2seq/encoder/gru/PartitionedCall]] [Op:__inference_train_function_45963]"
          ]
        }
      ],
      "source": [
        "PATH = MODEL_NAME\n",
        "if not(os.path.isdir(PATH)):\n",
        "        os.makedirs(os.path.join(PATH))\n",
        "\n",
        "checkpoint_path = MODEL_NAME + '.weights.h5'\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10,mode='max')\n",
        "\n",
        "history = model.fit([index_inputs, index_outputs], index_targets,\n",
        "                    batch_size=BATCH_SIZE, epochs=EPOCH,\n",
        "                    validation_split=VALIDATION_SPLIT, callbacks=[earlystop_callback, cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p0MHCykSs0a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tfdml_plugin",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
